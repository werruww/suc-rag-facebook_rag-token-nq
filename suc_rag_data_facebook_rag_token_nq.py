# -*- coding: utf-8 -*-
"""suc_rag_data_facebook_rag-token-nq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1umgZdanATwKKDhlHdyKAbd_k-HuWYzwQ
"""





!pip install langchain langchain-community faiss-cpu sentence-transformers transformers

!huggingface-cli login --token XXX

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFaceHub
from langchain.chains import RetrievalQA

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨
loader = TextLoader("book.txt", encoding="utf-8")
documents = loader.load()

# 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
)
texts = text_splitter.split_documents(documents)

# 3. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù†Ø§Ù‚Ø³ÙŠ
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# 4. Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS
vectorstore = FAISS.from_documents(texts, embeddings)

# 5. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
llm = HuggingFaceHub(
    repo_id="google/flan-t5-large",
    model_kwargs={"temperature": 0.5, "max_length": 512}
)

# 6. Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# 7. Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„
query = "what is Climate Change?"
result = qa({"query": query})

print("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:", result["result"])
print("Ø§Ù„Ù…ØµØ§Ø¯Ø±:")
for doc in result["source_documents"]:
    print("-", doc.metadata["source"], "ØµÙØ­Ø©", doc.metadata.get("page", "N/A"))

"""https://python.langchain.com/docs/integrations/vectorstores/"""

Menlo/Jan-nano

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡
def load_and_process_book(file_path):
    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()

    # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "Û”", " ", ""],  # ÙÙˆØ§ØµÙ„ Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
        chunk_size=500,
        chunk_overlap=100,
        length_function=len
    )

    return text_splitter.split_documents(documents)

# 2. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù†Ø§Ù‚Ø³ÙŠ
def create_embeddings():
    return HuggingFaceEmbeddings(
        model_name="aubmindlab/bert-base-arabertv02",  # Ù†Ù…ÙˆØ°Ø¬ Ø¹Ø±Ø¨ÙŠ Ù…Ù…ØªØ§Ø²
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
    )

# 3. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
def setup_arabic_llm():
    model_name = "aubmindlab/aragpt2-base"  # Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø¹Ø±Ø¨ÙŠ

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
    )

    # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù„Ù„ØªÙˆÙ„ÙŠØ¯
    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=300,
        temperature=0.3,
        top_p=0.95,
        repetition_penalty=1.15,
        device=0 if torch.cuda.is_available() else -1
    )

    return HuggingFacePipeline(pipeline=text_generation_pipeline)

# 4. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
def build_rag_system(file_path):
    # ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ
    texts = load_and_process_book(file_path)

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
    embeddings = create_embeddings()

    # Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS
    vectorstore = FAISS.from_documents(texts, embeddings)

    # ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    llm = setup_arabic_llm()

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True
    )

    return qa

# 5. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…
if __name__ == "__main__":
    # Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ÙƒØªØ§Ø¨ (Ø§Ø³ØªØ¨Ø¯Ù„Ù‡ Ø¨Ù…Ø³Ø§Ø± ÙƒØªØ§Ø¨Ùƒ)
    book_path = "book.txt"

    # Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    qa_system = build_rag_system(book_path)

    # ÙˆØ§Ø¬Ù‡Ø© Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø³ÙŠØ·Ø©
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ù„Ù„ÙƒØªØ¨ - ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ©")
    print("="*50 + "\n")

    while True:
        query = input("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨ (Ø£Ùˆ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡): ")

        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            break

        if query.strip() == "":
            continue

        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        result = qa_system({"query": query})

        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
        print(result["result"])

        print("\nØ§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
        for i, doc in enumerate(result["source_documents"], 1):
            print(f"{i}. {doc.page_content[:150]}...")

        print("\n" + "-"*50 + "\n")

Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Menlo/Jan-nano
Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§ÙÙƒØ§Ø± Ù…Ù†
https://python.langchain.com/docs/integrations/vectorstores/

"""### Ø´ØºØ§Ù„"""

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡
def load_and_process_book(file_path):
    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "Û”", " ", ""],
        chunk_size=500,
        chunk_overlap=100,
        length_function=len
    )

    return text_splitter.split_documents(documents)

# 2. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù†Ø§Ù‚Ø³ÙŠ
def create_embeddings():
    # Ø¥Ø²Ø§Ù„Ø© device Ù…Ù† Ù‡Ù†Ø§ Ù„Ø£Ù† accelerate ÙŠØªÙˆÙ„Ù‰ Ø°Ù„Ùƒ
    return HuggingFaceEmbeddings(
        model_name="aubmindlab/bert-base-arabertv02"
    )

# 3. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
def setup_arabic_llm():
    model_name = "aubmindlab/aragpt2-base"

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙˆÙ† device_map Ù„Ù„ØªØ­ÙƒÙ… Ø§Ù„ÙŠØ¯ÙˆÙŠ
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )

    # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ ÙŠØ¯ÙˆÙŠÙ‹Ø§
    device = 0 if torch.cuda.is_available() else -1
    model = model.to(torch.device("cuda" if device == 0 else "cpu"))

    # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=300,
        temperature=0.3,
        top_p=0.95,
        repetition_penalty=1.15,
        device=device  # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø² Ù‡Ù†Ø§
    )

    return HuggingFacePipeline(pipeline=text_generation_pipeline)

# 4. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
def build_rag_system(file_path):
    texts = load_and_process_book(file_path)
    embeddings = create_embeddings()
    vectorstore = FAISS.from_documents(texts, embeddings)
    llm = setup_arabic_llm()

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True
    )

    return qa

# 5. ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…
if __name__ == "__main__":
    book_path = "book.txt"
    qa_system = build_rag_system(book_path)

    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ù„Ù„ÙƒØªØ¨ - ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ©")
    print("="*50 + "\n")

    while True:
        query = input("Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨ (Ø£Ùˆ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡): ")

        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            break

        if query.strip() == "":
            continue

        result = qa_system({"query": query})

        print("\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
        print(result["result"])

        print("\nØ§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
        for i, doc in enumerate(result["source_documents"], 1):
            print(f"{i}. {doc.page_content[:150]}...")

        print("\n" + "-"*50 + "\n")

facebook/rag-token-nq

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch
import os

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
class Config:
    EMBEDDING_MODEL = "aubmindlab/bert-base-arabertv02"
    LLM_MODEL = "aubmindlab/aragpt2-base"
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    MAX_NEW_TOKENS = 350
    TEMPERATURE = 0.4
    RETRIEVE_K = 4
    SEPARATORS = ["\n\n", "\n", "Û”", " ", ""]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡
def load_and_process_book(file_path):
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        text_splitter = RecursiveCharacterTextSplitter(
            separators=Config.SEPARATORS,
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False
        )

        return text_splitter.split_documents(documents)

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}")
        return None

# 2. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù†Ø§Ù‚Ø³ÙŠ
def create_embeddings():
    try:
        return HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': Config.DEVICE}
        )
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {str(e)}")
        return None

# 3. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
def setup_arabic_llm():
    try:
        tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL)

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        model = AutoModelForCausalLM.from_pretrained(
            Config.LLM_MODEL,
            torch_dtype=torch_dtype
        )

        # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        model.to(Config.DEVICE)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=Config.MAX_NEW_TOKENS,
            temperature=Config.TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.15,
            device=0 if Config.DEVICE == "cuda" else -1
        )

        return HuggingFacePipeline(pipeline=text_generation_pipeline)

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {str(e)}")
        return None

# 4. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
def build_rag_system(file_path):
    # ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡...")
    texts = load_and_process_book(file_path)
    if not texts:
        return None

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
    embeddings = create_embeddings()
    if not embeddings:
        return None

    # Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS
    print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«...")
    vectorstore = FAISS.from_documents(texts, embeddings)

    # ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...")
    llm = setup_arabic_llm()
    if not llm:
        return None

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": Config.RETRIEVE_K}),
        return_source_documents=True
    )

    return qa

# 5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
def run_interface(qa_system):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ù„Ù„ÙƒØªØ¨ - ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ©")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª' Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n")

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")

        # Ø§Ù„Ø®Ø±ÙˆØ¬ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù…
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø´ÙƒØ±Ø§Ù‹ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break

        # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        if query.lower() == "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª":
            show_settings()
            continue

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ§Ø±ØºØ©
        if query.strip() == "":
            print("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„ ØµØ§Ù„Ø­")
            continue

        try:
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            result = qa_system({"query": query})

            # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
            print(result["result"])

            # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±
            print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
            for i, doc in enumerate(result["source_documents"], 1):
                source = doc.metadata.get("source", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")
                content = doc.page_content.replace("\n", " ").strip()
                print(f"{i}. [{source}] - {content[:120]}...")

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
def show_settings():
    print("\nØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:")
    print(f"â€¢ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {Config.EMBEDDING_MODEL}")
    print(f"â€¢ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {Config.LLM_MODEL}")
    print(f"â€¢ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø©: {Config.CHUNK_SIZE}")
    print(f"â€¢ Ø§Ù„ØªØ¯Ø§Ø®Ù„: {Config.CHUNK_OVERLAP}")
    print(f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©: {Config.RETRIEVE_K}")
    print(f"â€¢ Ø§Ù„Ø¬Ù‡Ø§Ø²: {Config.DEVICE}")

    # Ø§Ù‚ØªØ±Ø§Ø­ ØªØºÙŠÙŠØ±Ø§Øª Ù„Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¶Ø¹ÙŠÙØ©
    if Config.DEVICE == "cpu":
        print("\nÙ†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¶Ø¹ÙŠÙØ©:")
        print("â€¢ ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø© (CHUNK_SIZE) Ø¥Ù„Ù‰ 300")
        print("â€¢ ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© (RETRIEVE_K) Ø¥Ù„Ù‰ 2")
        print("â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø£ØµØºØ± Ù…Ø«Ù„ 'aubmindlab/aragpt2-small'")

# 6. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    # Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ÙƒØªØ§Ø¨ (ØªØºÙŠÙŠØ±Ù‡ Ù„Ù…Ø³Ø§Ø± ÙƒØªØ§Ø¨Ùƒ)
    book_path = "path/to/your/book.txt"

    # Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    qa_system = build_rag_system(book_path)

    if qa_system:
        run_interface(qa_system)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    torch.set_grad_enabled(False)
    if Config.DEVICE == "cpu":
        torch.set_num_threads(4)

    main()

class Config:
    EMBEDDING_MODEL = "aubmindlab/bert-base-arabertv02"
    LLM_MODEL = "aubmindlab/aragpt2-base"

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch
import os

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
class Config:
    EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v1"
    LLM_MODEL = "facebook/rag-token-nq"
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    MAX_NEW_TOKENS = 350
    TEMPERATURE = 0.4
    RETRIEVE_K = 4
    SEPARATORS = ["\n\n", "\n", "Û”", " ", ""]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡
def load_and_process_book(file_path):
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Øµ
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        text_splitter = RecursiveCharacterTextSplitter(
            separators=Config.SEPARATORS,
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False
        )

        return text_splitter.split_documents(documents)

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}")
        return None

# 2. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù†Ø§Ù‚Ø³ÙŠ
def create_embeddings():
    try:
        return HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': Config.DEVICE}
        )
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {str(e)}")
        return None

# 3. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ
def setup_arabic_llm():
    try:
        tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL)

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        model = AutoModelForCausalLM.from_pretrained(
            Config.LLM_MODEL,
            torch_dtype=torch_dtype
        )

        # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        model.to(Config.DEVICE)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=Config.MAX_NEW_TOKENS,
            temperature=Config.TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.15,
            device=0 if Config.DEVICE == "cuda" else -1
        )

        return HuggingFacePipeline(pipeline=text_generation_pipeline)

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {str(e)}")
        return None

# 4. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
def build_rag_system(file_path):
    # ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡...")
    texts = load_and_process_book(file_path)
    if not texts:
        return None

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
    embeddings = create_embeddings()
    if not embeddings:
        return None

    # Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS
    print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«...")
    vectorstore = FAISS.from_documents(texts, embeddings)

    # ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...")
    llm = setup_arabic_llm()
    if not llm:
        return None

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": Config.RETRIEVE_K}),
        return_source_documents=True
    )

    return qa

# 5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
def run_interface(qa_system):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ù„Ù„ÙƒØªØ¨ - ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ©")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª' Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n")

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")

        # Ø§Ù„Ø®Ø±ÙˆØ¬ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù…
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø´ÙƒØ±Ø§Ù‹ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break

        # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        if query.lower() == "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª":
            show_settings()
            continue

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ§Ø±ØºØ©
        if query.strip() == "":
            print("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„ ØµØ§Ù„Ø­")
            continue

        try:
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            result = qa_system({"query": query})

            # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
            print(result["result"])

            # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±
            print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
            for i, doc in enumerate(result["source_documents"], 1):
                source = doc.metadata.get("source", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")
                content = doc.page_content.replace("\n", " ").strip()
                print(f"{i}. [{source}] - {content[:120]}...")

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
def show_settings():
    print("\nØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:")
    print(f"â€¢ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {Config.EMBEDDING_MODEL}")
    print(f"â€¢ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {Config.LLM_MODEL}")
    print(f"â€¢ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø©: {Config.CHUNK_SIZE}")
    print(f"â€¢ Ø§Ù„ØªØ¯Ø§Ø®Ù„: {Config.CHUNK_OVERLAP}")
    print(f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©: {Config.RETRIEVE_K}")
    print(f"â€¢ Ø§Ù„Ø¬Ù‡Ø§Ø²: {Config.DEVICE}")

    # Ø§Ù‚ØªØ±Ø§Ø­ ØªØºÙŠÙŠØ±Ø§Øª Ù„Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¶Ø¹ÙŠÙØ©
    if Config.DEVICE == "cpu":
        print("\nÙ†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¶Ø¹ÙŠÙØ©:")
        print("â€¢ ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø© (CHUNK_SIZE) Ø¥Ù„Ù‰ 300")
        print("â€¢ ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© (RETRIEVE_K) Ø¥Ù„Ù‰ 2")
        print("â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø£ØµØºØ± Ù…Ø«Ù„ 'aubmindlab/aragpt2-small'")

# 6. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    # Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ÙƒØªØ§Ø¨ (ØªØºÙŠÙŠØ±Ù‡ Ù„Ù…Ø³Ø§Ø± ÙƒØªØ§Ø¨Ùƒ)
    book_path = "book.txt"

    # Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    qa_system = build_rag_system(book_path)

    if qa_system:
        run_interface(qa_system)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    torch.set_grad_enabled(False)
    if Config.DEVICE == "cpu":
        torch.set_num_threads(4)

    main()

Qwen/Qwen3-Embedding-0.6B

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch
import os

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… - Ù…Ø¹Ø¯Ù„Ø© Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ© ØªØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹
class Config:
    EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v1"  # Ù†Ù…ÙˆØ°Ø¬ ØªØ¶Ù…ÙŠÙ† Ø¹Ø±Ø¨ÙŠ
    LLM_MODEL = "facebook/rag-token-nq"  # Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø¹Ø±Ø¨ÙŠ
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    MAX_NEW_TOKENS = 350
    TEMPERATURE = 0.4
    RETRIEVE_K = 4
    SEPARATORS = ["\n\n", "\n", "Û”", " ", ""]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")

        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            separators=Config.SEPARATORS,
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False
        )

        return text_splitter.split_documents(documents)

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}")
        return None

# 2. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ…Ø«ÙŠÙ„ Ù†Ø§Ù‚Ø³ÙŠ - Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
def create_embeddings():
    try:
        return HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': Config.DEVICE, 'trust_remote_code': True}
        )
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {str(e)}")
        return None

# 3. ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø±Ø¨ÙŠ - Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©
def setup_arabic_llm():
    try:
        tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL)

        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        model = AutoModelForCausalLM.from_pretrained(
            Config.LLM_MODEL,
            torch_dtype=torch_dtype
        )

        # Ù†Ù‚Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
        device = torch.device(Config.DEVICE)
        model.to(device)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø² Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=Config.MAX_NEW_TOKENS,
            temperature=Config.TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.15,
            device=device.index if device.type == "cuda" else -1
        )

        return HuggingFacePipeline(pipeline=text_generation_pipeline)

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {str(e)}")
        return None

# 4. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
def build_rag_system(file_path):
    # ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†Øµ
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆØªÙ‚Ø³ÙŠÙ…Ù‡...")
    texts = load_and_process_book(file_path)
    if not texts:
        return None

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†...")
    embeddings = create_embeddings()
    if not embeddings:
        return None

    # Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS
    print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø«...")
    vectorstore = FAISS.from_documents(texts, embeddings)

    # ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯...")
    llm = setup_arabic_llm()
    if not llm:
        return None

    # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": Config.RETRIEVE_K}),
        return_source_documents=True
    )

    return qa

# 5. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠØ©
def run_interface(qa_system):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ù„Ù„ÙƒØªØ¨ - ÙŠØ¹Ù…Ù„ Ù…Ø­Ù„ÙŠØ§Ù‹ Ø¨Ù†Ù…Ø§Ø°Ø¬ Ø¹Ø±Ø¨ÙŠØ©")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª' Ù„ØªØºÙŠÙŠØ± Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª\n")

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")

        # Ø§Ù„Ø®Ø±ÙˆØ¬ Ù…Ù† Ø§Ù„Ù†Ø¸Ø§Ù…
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø´ÙƒØ±Ø§Ù‹ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ùƒ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break

        # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        if query.lower() == "Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª":
            show_settings()
            continue

        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ÙØ§Ø±ØºØ©
        if query.strip() == "":
            print("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„ ØµØ§Ù„Ø­")
            continue

        try:
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
            result = qa_system({"query": query})

            # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
            print(result["result"])

            # Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø±
            print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©:")
            for i, doc in enumerate(result["source_documents"], 1):
                source = doc.metadata.get("source", "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")
                content = doc.page_content.replace("\n", " ").strip()
                print(f"{i}. [{source}] - {content[:120]}...")

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
def show_settings():
    print("\nØ§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©:")
    print(f"â€¢ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ†: {Config.EMBEDDING_MODEL}")
    print(f"â€¢ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {Config.LLM_MODEL}")
    print(f"â€¢ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø©: {Config.CHUNK_SIZE}")
    print(f"â€¢ Ø§Ù„ØªØ¯Ø§Ø®Ù„: {Config.CHUNK_OVERLAP}")
    print(f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©: {Config.RETRIEVE_K}")
    print(f"â€¢ Ø§Ù„Ø¬Ù‡Ø§Ø²: {Config.DEVICE}")

    # Ø§Ù‚ØªØ±Ø§Ø­ ØªØºÙŠÙŠØ±Ø§Øª Ù„Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¶Ø¹ÙŠÙØ©
    if Config.DEVICE == "cpu":
        print("\nÙ†ØµØ§Ø¦Ø­ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¶Ø¹ÙŠÙØ©:")
        print("â€¢ ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø·Ø¹Ø© (CHUNK_SIZE) Ø¥Ù„Ù‰ 300")
        print("â€¢ ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© (RETRIEVE_K) Ø¥Ù„Ù‰ 2")
        print("â€¢ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø§Ø°Ø¬ Ø£ØµØºØ± Ù…Ø«Ù„ 'aubmindlab/aragpt2-small'")

# 6. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    # Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„ÙƒØªØ§Ø¨ (ØªØºÙŠÙŠØ±Ù‡ Ù„Ù…Ø³Ø§Ø± ÙƒØªØ§Ø¨Ùƒ)
    book_path = "book.txt"

    # Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
    qa_system = build_rag_system(book_path)

    if qa_system:
        run_interface(qa_system)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    torch.set_grad_enabled(False)
    if Config.DEVICE == "cpu":
        torch.set_num_threads(4)

    main()







!pip install "numpy<2.0"

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø§Øª Hugging Face Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø¨Ù†Ø§Ø¡ RAG
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk, load_dataset

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    # Ù†Ù…Ø§Ø°Ø¬ RAG Ø§Ù„Ù…Ø¯Ù…Ø¬Ø© - Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…Ø§Ø°Ø¬ Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ RAG
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base" # Ù‡Ø°Ø§ Ù‡Ùˆ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù…Ù‡ RAG Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹

    CHUNK_SIZE = 400  # RAG ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ù…Ø¹ Ù‚Ø·Ø¹ Ø£ØµØºØ± Ù‚Ù„ÙŠÙ„Ø§Ù‹
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    FAISS_INDEX_PATH = "./my_faiss_index" # Ù…Ø³Ø§Ø± Ù„Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡ (Ù†ÙØ³ Ø§Ù„Ø¯Ø§Ù„Ø©ØŒ Ù„Ø§ ØªØºÙŠÙŠØ±)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")

        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
        )

        # Ø³Ù†Ø¹ÙŠØ¯Ù‡Ø§ ÙƒÙ†ØµÙˆØµ Ø®Ø§Ù… Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† ÙƒØ§Ø¦Ù†Ø§Øª Document Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¹ datasets
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]

    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}")
        return None

# 2. Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ RAG
def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)

    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù‚Ø§Ù…ÙˆØ³ ÙŠÙ…ÙƒÙ† Ù„Ù…ÙƒØªØ¨Ø© datasets ÙÙ‡Ù…Ù‡
    dataset_dict = {
        "title": ["my_book"] * len(docs), # RAG ÙŠØªÙˆÙ‚Ø¹ Ø¹Ù†ÙˆØ§Ù†Ø§Ù‹ ÙˆÙ†ØµØ§Ù‹
        "text": docs
    }
    dataset = Dataset.from_dict(dataset_dict)

    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ DPR
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy()},
        batched=True,
        batch_size=16 # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¹Ø¯ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø±Ù‚Ù… Ø­Ø³Ø¨ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù€ GPU
    )

    print(f"Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ­ÙØ¸Ù‡ ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
    dataset.add_faiss_index(column="embeddings", index_name="my_index")
    dataset.save_faiss_index('my_index', Config.FAISS_INDEX_PATH)

    return dataset

# 3. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª Hugging Face
def build_hf_rag_system(file_path):
    # ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨
    docs = load_and_process_book(file_path)
    if not docs:
        return None, None

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ RAG
    # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ Ø¨Ø§Ù„ÙØ¹Ù„ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„Ù‡ Ù„ØªÙˆÙÙŠØ± Ø§Ù„ÙˆÙ‚Øª
    if os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙÙ‡Ø±Ø³ Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø³ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ù…Ù† '{Config.FAISS_INDEX_PATH}'")
        rag_dataset = Dataset.load_from_disk(os.path.dirname(Config.FAISS_INDEX_PATH))
        rag_dataset.load_faiss_index('my_index', Config.FAISS_INDEX_PATH)
    else:
        rag_dataset = build_rag_compatible_index(docs)

    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù€ Retriever Ù„ÙŠØ¹Ù…Ù„ Ù…Ø¹ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø°ÙŠ Ø¨Ù†ÙŠÙ†Ø§Ù‡
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )

    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù€ Tokenizer
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)

    # ØªÙ‡ÙŠØ¦Ø© Ù†Ù…ÙˆØ°Ø¬ RAG Ø§Ù„ÙƒØ§Ù…Ù„ (Ù…ÙˆÙ„Ø¯ + Ù…Ø³ØªØ±Ø¬Ø¹)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)

    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¨Ø³Ø·Ø©)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break

        if not query.strip():
            continue

        try:
            # 1. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¥Ù„Ù‰ ØªÙˆÙƒÙ†Ø²
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)

            # 2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            # RAG ÙŠÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªÙˆÙ„ÙŠØ¯ ÙÙŠ Ø®Ø·ÙˆØ© ÙˆØ§Ø­Ø¯Ø©
            output_ids = model.generate(input_ids=inputs["input_ids"])

            # 3. ÙÙƒ ØªØ´ÙÙŠØ± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
            print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±) ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # --- ØªØºÙŠÙŠØ± Ù‡Ù†Ø§: Ù…Ø³Ø§Ø± Ù„Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„ Ø³ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ Ù…Ø¹Ø§Ù‹ ---
    DATASET_WITH_INDEX_PATH = "./my_rag_dataset_directory"

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø©)
def load_and_process_book(file_path):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
        )
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}")
        return None


# (ØªØºÙŠÙŠØ± Ø¨Ø³ÙŠØ·: Ø¥Ø²Ø§Ù„Ø© Ø³Ø·Ø± Ø§Ù„Ø­ÙØ¸ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø©)
def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True,
        batch_size=16
    )
    print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©...")
    # Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙÙ‡Ø±Ø³ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    dataset.add_faiss_index(column="embeddings")
    # Ù„Ø§ Ù†Ø­ÙØ¸ Ù‡Ù†Ø§ØŒ Ø³Ù†Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    return dataset

# 3. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ø¬ÙˆÙ‡Ø±ÙŠ ÙÙŠ Ù…Ù†Ø·Ù‚ Ø§Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„)
def build_hf_rag_system(file_path):
    docs = load_and_process_book(file_path)
    if not docs:
        return None, None

    # --- ØªØºÙŠÙŠØ± Ø¬ÙˆÙ‡Ø±ÙŠ Ù‡Ù†Ø§ ---
    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³
    if os.path.exists(Config.DATASET_WITH_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.DATASET_WITH_INDEX_PATH}'...")
        # Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙƒØ§Ø¦Ù† Dataset Ø¨Ø£ÙƒÙ…Ù„Ù‡ Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯
        rag_dataset = Dataset.load_from_disk(Config.DATASET_WITH_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ø¬Ø¯ÙŠØ¯...")
        # Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ Ù…Ù† Ø§Ù„ØµÙØ±
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ '{Config.DATASET_WITH_INDEX_PATH}'...")
        # Ù‚Ù… Ø¨Ø­ÙØ¸ ÙƒØ§Ø¦Ù† Dataset Ø¨Ø£ÙƒÙ…Ù„Ù‡ (Ù…Ø¹ Ø§Ù„ÙÙ‡Ø±Ø³) ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯
        rag_dataset.save_to_disk(Config.DATASET_WITH_INDEX_PATH)
        print("ØªÙ… Ø§Ù„Ø­ÙØ¸ Ø¨Ù†Ø¬Ø§Ø­.")

    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset  # Ø§Ù„Ø¢Ù† rag_dataset ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø´ÙƒÙ„ Ù…Ø¤ÙƒØ¯
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)
def run_interface(model, tokenizer):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        if not query.strip():
            continue
        try:
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:")
            print(answer.strip())
        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")


def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)
    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    # Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ù„ Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (ØªØ®ÙÙŠØ¶ numpy) ÙØªØ£ÙƒØ¯ Ø£Ù†Ù‡ Ù„Ø§ ÙŠØ²Ø§Ù„ Ù…Ø«Ø¨ØªØ§Ù‹
    # pip install "numpy<2.0"
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… (Ù…Ø¹ Ù…Ø³Ø§Ø±ÙŠÙ† Ù…Ù†ÙØµÙ„ÙŠÙ†) ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # --- ØªØºÙŠÙŠØ± Ù‡Ù†Ø§: Ù…Ø³Ø§Ø± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ…Ø³Ø§Ø± Ù…Ù†ÙØµÙ„ Ù„Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³ ---
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index" # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ø³Ù… Ù…Ù„Ù

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø§ØªÙŠÙ† Ø§Ù„Ø¯Ø§Ù„ØªÙŠÙ†)
def load_and_process_book(file_path):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    # Ù„Ø§ Ù†Ø¶ÙŠÙ Ø§Ù„ÙÙ‡Ø±Ø³ Ù‡Ù†Ø§ØŒ Ø³Ù†Ø¶ÙŠÙÙ‡ ÙÙŠ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡
    return dataset

# 3. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG (Ù…Ø¹ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù†ÙØµÙ„)
def build_hf_rag_system(file_path):
    # --- ØªØºÙŠÙŠØ± Ø¬ÙˆÙ‡Ø±ÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†Ø·Ù‚ ---
    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙˆØ§Ù„ÙÙ‡Ø±Ø³ Ù…Ø¹Ø§Ù‹
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        # 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs:
            return None, None

        # 1. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
        rag_dataset = build_rag_compatible_index(docs)

        # 2. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (Ø¨Ø¯ÙˆÙ† Ø§Ù„ÙÙ‡Ø±Ø³)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)

        # 3. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙÙ‡Ø±Ø³ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")

        # 4. Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")

    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)
def run_interface(model, tokenizer):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    print("\n" + "="*50); print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`"); print("="*50); print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")
    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]: print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!"); break
        if not query.strip(): continue
        try:
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"); print(answer.strip())
        except Exception as e: print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)
    if model and tokenizer: run_interface(model, tokenizer)
    else: print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

# (Keep all other functions the same)

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    # The retriever is part of the model object
    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break

        if not query.strip():
            continue

        try:
            # --- DEBUGGING PART: Let's see what the retriever finds ---
            print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

            # 1. First, encode the question for the retriever
            question_inputs = tokenizer(query, return_tensors="pt")
            question_input_ids = question_inputs['input_ids'].to(Config.DEVICE)

            # 2. Use the retriever to get the context documents
            retrieved_docs_dict = retriever(
                question_input_ids,
                n_docs=4  # Let's retrieve 4 docs to see what it finds
            )

            # 3. Decode and print the retrieved documents for inspection
            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§:")
            for i, text in enumerate(retrieved_texts):
                print(f"  {i+1}. {text.strip().replace('  ', ' ')}")
            # --- END OF DEBUGGING PART ---


            # Now, let the model generate the answer as before
            print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...")

            # The model internally does the same retrieval, but we did it manually to see
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])

            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
            print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# (Keep the main function the same)
def main():
    # ...

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… (Ù…Ø¹ Ù…Ø³Ø§Ø±ÙŠÙ† Ù…Ù†ÙØµÙ„ÙŠÙ†) ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # --- ØªØºÙŠÙŠØ± Ù‡Ù†Ø§: Ù…Ø³Ø§Ø± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ…Ø³Ø§Ø± Ù…Ù†ÙØµÙ„ Ù„Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³ ---
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index" # ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ø§Ø³Ù… Ù…Ù„Ù

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø§ØªÙŠÙ† Ø§Ù„Ø¯Ø§Ù„ØªÙŠÙ†)
def load_and_process_book(file_path):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    # Ù„Ø§ Ù†Ø¶ÙŠÙ Ø§Ù„ÙÙ‡Ø±Ø³ Ù‡Ù†Ø§ØŒ Ø³Ù†Ø¶ÙŠÙÙ‡ ÙÙŠ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡
    return dataset

# 3. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG (Ù…Ø¹ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø­ÙØ¸ ÙˆØ§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù†ÙØµÙ„)
def build_hf_rag_system(file_path):
    # --- ØªØºÙŠÙŠØ± Ø¬ÙˆÙ‡Ø±ÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ù†Ø·Ù‚ ---
    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙˆØ§Ù„ÙÙ‡Ø±Ø³ Ù…Ø¹Ø§Ù‹
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        # 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs:
            return None, None

        # 1. Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
        rag_dataset = build_rag_compatible_index(docs)

        # 2. Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… (Ø¨Ø¯ÙˆÙ† Ø§Ù„ÙÙ‡Ø±Ø³)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)

        # 3. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙÙ‡Ø±Ø³ Ø¥Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")

        # 4. Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ÙÙ‡Ø±Ø³ Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")

    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© ÙˆØ§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©)# (Keep all other functions the same)

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    # The retriever is part of the model object
    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break

        if not query.strip():
            continue

        try:
            # --- DEBUGGING PART: Let's see what the retriever finds ---
            print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

            # 1. First, encode the question for the retriever
            question_inputs = tokenizer(query, return_tensors="pt")
            question_input_ids = question_inputs['input_ids'].to(Config.DEVICE)

            # 2. Use the retriever to get the context documents
            retrieved_docs_dict = retriever(
                question_input_ids,
                n_docs=4  # Let's retrieve 4 docs to see what it finds
            )

            # 3. Decode and print the retrieved documents for inspection
            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§:")
            for i, text in enumerate(retrieved_texts):
                print(f"  {i+1}. {text.strip().replace('  ', ' ')}")
            # --- END OF DEBUGGING PART ---


            # Now, let the model generate the answer as before
            print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©...")

            # The model internally does the same retrieval, but we did it manually to see
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])

            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
            print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# (Keep the main function the same)
def main():

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ§Ø¨ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP
        )
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}")
        return None

# 2. Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ RAG
def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True,
        batch_size=16
    )
    return dataset

# 3. Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG
def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs:
            return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")

    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ ØªØ¹Ø¯ÙŠÙ„ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    # The retriever is part of the model object, let's get it for debugging
    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break

        if not query.strip():
            continue

        try:
            # --- DEBUGGING PART: Let's see what the retriever finds ---
            print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

            # 1. First, encode the question for the retriever
            question_inputs = tokenizer(query, return_tensors="pt")
            question_input_ids = question_inputs['input_ids'].to(Config.DEVICE)

            # 2. Use the retriever to get the context documents
            # The n_docs parameter controls how many documents to retrieve.
            retrieved_docs_dict = retriever(
                question_input_ids,
                n_docs=4
            )

            # 3. Decode and print the retrieved documents for inspection
            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ (Context):")
            for i, text in enumerate(retrieved_texts):
                # Clean up the text for better readability
                clean_text = ' '.join(text.strip().split())
                print(f"  {i+1}. {clean_text}")
            # --- END OF DEBUGGING PART ---


            # Now, let the model generate the answer as before
            print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")

            # We already have the encoded inputs from the debugging step
            # but we call it again to keep the logic simple, as the model does this internally.
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])

            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
            print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")


# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    # Make sure your book.txt contains English text
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„)
def load_and_process_book(file_path):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    # ... Ù†ÙØ³ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚ ...
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        if not query.strip():
            continue

        try:
            print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

            # 1. ØªØ±Ù…ÙŠØ² Ø§Ù„Ø³Ø¤Ø§Ù„
            question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            question_input_ids = question_inputs['input_ids']

            # 2. Ø­Ø³Ø§Ø¨ Ø§Ù„Ù€ embedding Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„Ø³Ø¤Ø§Ù„ (Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªÙŠ ÙƒØ§Ù†Øª Ù†Ø§Ù‚ØµØ©)
            # Ù†Ø­Ù† Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø´ÙØ± Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¯Ù…Ø¬ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            question_encoder_outputs = model.rag.question_encoder(question_input_ids)
            question_hidden_states = question_encoder_outputs.pooler_output

            # 3. Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹ Ø¨Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©ØŒ Ù…Ø¹ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù€ IDs Ùˆ Ø§Ù„Ù€ embedding
            retrieved_docs_dict = retriever(
                question_input_ids=question_input_ids,
                question_hidden_states=question_hidden_states,
                n_docs=4 # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø§Ø¯ Ø§Ø³ØªØ±Ø¬Ø§Ø¹Ù‡Ø§
            )

            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ (Context):")
            for i, text in enumerate(retrieved_texts):
                clean_text = ' '.join(text.strip().split())
                print(f"  {i+1}. {clean_text}")

            print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
            output_ids = model.generate(input_ids=question_input_ids)
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
            print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ pooler_output)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„ØµØ­ÙŠØ­ Ù‡Ù†Ø§ ---
                # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…Ø´ÙØ± Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
                question_encoder_outputs = model.rag.question_encoder(question_input_ids)
                # Ø§Ù„Ù€ pooler_output Ù‡Ùˆ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø«Ø§Ù†ÙŠ (ÙÙ‡Ø±Ø³ 1) ÙÙŠ Ø§Ù„Ù€ tuple Ø§Ù„Ù…Ø±Ø¬Ø¹
                question_hidden_states = question_encoder_outputs[1]
                # -----------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
                print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")


# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„ØµØ­ÙŠØ­)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù‡Ù†Ø§ ---
                # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ù…Ø´ÙØ± Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
                question_encoder_outputs = model.rag.question_encoder(question_input_ids)
                # Ø§Ù„Ù…Ø®Ø±Ø¬ Ù‡Ùˆ tuple Ø¨Ø¹Ù†ØµØ± ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·ØŒ ÙˆÙ‡Ùˆ Ø§Ù„Ù€ embedding Ø§Ù„Ø°ÙŠ Ù†Ø­ØªØ§Ø¬Ù‡.
                question_hidden_states = question_encoder_outputs[0]
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
                print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")


# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

import torch
import os
import numpy as np # Import numpy, it's good practice
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Tensor Ø¥Ù„Ù‰ NumPy)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                question_encoder_outputs = model.rag.question_encoder(question_input_ids)

                # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù‡Ù†Ø§ ---
                # ØªØ­ÙˆÙŠÙ„ Ù…Ø®Ø±Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ù† Tensor Ø¹Ù„Ù‰ GPU Ø¥Ù„Ù‰ NumPy array Ø¹Ù„Ù‰ CPU
                question_hidden_states = question_encoder_outputs[0].cpu().numpy()
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
                print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

who founded the walt disney company?
(Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: Walt and Roy O. Disney)
who was the commander of the apollo 11 mission?
(Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: Neil Armstrong)
where is the amazon rainforest located?
(Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: South America)
who won the first fifa world cup tournament?
(Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: Uruguay)
what process produces most of the planet's oxygen?
(Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©: Photosynthesis)

!rm -r my_rag_raw_dataset

!rmdir /s /q my_rag_raw_dataset

/content/book.txt


The Walt Disney Company, commonly known as Disney, is an American multinational mass media and entertainment conglomerate. Its headquarters are located at the Walt Disney Studios in Burbank, California. Disney was founded on October 16, 1923, by brothers Walt and Roy O. Disney.

The Apollo 11 mission was the first crewed mission to land on the Moon. It was launched by a Saturn V rocket from Kennedy Space Center in Florida on July 16, 1969. The crew consisted of Commander Neil Armstrong, Lunar Module Pilot Buzz Aldrin, and Command Module Pilot Michael Collins.

The Amazon rainforest is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest.

The FIFA World Cup is an international association football competition contested by the senior men's national teams of the members of the FÃ©dÃ©ration Internationale de Football Association (FIFA). The first tournament was held in 1930 and was won by Uruguay.

Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy, through a cellular process that is stored for later use. This process is crucial for life on Earth as it is responsible for producing most of the planet's oxygen.

"""### Ø´ØºØ§Ù„ Ø¬ÙŠØ¯"""

import torch
import os
import numpy as np # Import numpy, it's good practice
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Tensor Ø¥Ù„Ù‰ NumPy)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                question_encoder_outputs = model.rag.question_encoder(question_input_ids)

                # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù‡Ù†Ø§ ---
                # ØªØ­ÙˆÙŠÙ„ Ù…Ø®Ø±Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ù† Tensor Ø¹Ù„Ù‰ GPU Ø¥Ù„Ù‰ NumPy array Ø¹Ù„Ù‰ CPU
                question_hidden_states = question_encoder_outputs[0].cpu().numpy()
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
                print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()













from datasets import load_dataset
import json

def create_squad_subset(output_filename="squad_book.txt", num_articles=50):
    """
    Downloads the SQuAD dataset and creates a small text file from it.
    Each entry will contain the context, question, and answer.
    """
    print("Downloading SQuAD dataset...")
    # Load the training split of the SQuAD dataset
    squad_dataset = load_dataset("squad", split="train")

    print(f"Processing the first {num_articles} articles...")

    # Use a set to keep track of contexts we've already added
    # to avoid duplicates and keep the content diverse.
    processed_contexts = set()

    with open(output_filename, "w", encoding="utf-8") as f:
        for item in squad_dataset:
            context = item['context']

            # If we haven't seen this context before
            if context not in processed_contexts:
                # Add the context to our file
                f.write("CONTEXT: " + context.strip() + "\n\n")

                # Find all questions related to this context
                related_questions = [
                    q for q in squad_dataset
                    if q['context'] == context
                ]

                # Add all related questions and answers
                for q_item in related_questions:
                    question = q_item['question']
                    answer = q_item['answers']['text'][0] # Get the first answer

                    f.write(f"QUESTION: {question.strip()}\n")
                    f.write(f"ANSWER: {answer.strip()}\n\n")

                f.write("="*80 + "\n\n") # Separator for readability

                processed_contexts.add(context)

            # Stop when we have processed enough unique articles
            if len(processed_contexts) >= num_articles:
                break

    print(f"Successfully created '{output_filename}' with content from {len(processed_contexts)} articles.")

# --- Run the function ---
# You can change num_articles to get a smaller or larger file.
# 50 articles should be around 1-2 MB.
create_squad_subset(num_articles=50)

Ø´ØºÙ‘Ù„ ÙˆØ¬Ø±Ù‘Ø¨: Ø´ØºÙ‘Ù„ Ø§Ù„ÙƒÙˆØ¯. Ø¹Ù†Ø¯Ù…Ø§ ÙŠØµØ¨Ø­ Ø¬Ø§Ù‡Ø²Ù‹Ø§ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø·Ø±Ø­ Ø£Ø³Ø¦Ù„Ø© Ù…Ø«Ù„:
What is used to measure a supercomputer's performance?
Who develops the PlayStation?
When was the College of Commerce established?

squad_book.txt

/content/squad_book.txt



CONTEXT: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.

QUESTION: What is in front of the Main Building?
ANSWER: a copper statue of Christ

QUESTION: What is the Grotto at Notre Dame?
ANSWER: a Marian place of prayer and reflection

QUESTION: What sits on top of the Main Building at Notre Dame?
ANSWER: a golden statue of the Virgin Mary

================================================================================

CONTEXT: The University of Notre Dame du Lac is a Catholic research university located in Notre Dame, Indiana, an unincorporated community north of the city of South Bend, in St. Joseph County, Indiana. The main campus covers 1,250 acres in a suburban setting and contains a number of recognizable landmarks, such as the Golden Dome, the "Word of Life" mural (commonly known as Touchdown Jesus), Notre Dame Stadium, and the Basilica. The school was founded on November 26, 1842, by Father Edward Sorin, CSC, who was also its first president.

QUESTION: Who founded Notre Dame?
ANSWER: Father Edward Sorin, CSC

QUESTION: What is the official name of the mural that is commonly known as "Touchdown Jesus"?
ANSWER: "Word of Life"

QUESTION: When was Notre Dame founded?
ANSWER: November 26, 1842

================================================================================

CONTEXT: As of 2014, the university is organized into seven components. It consists of the College of Arts and Letters, the College of Science, the Mendoza College of Business, the College of Engineering, the College of Architecture, the Law School, and the Graduate School. The College of Arts and Letters, founded in conjunction with the university in 1842, is the largest of the colleges. The university's graduate programs were reorganized in 1918 and the Graduate School was established. The first professional degree offered at the university was a law degree (LL.B.) in 1869, and the School of Law was established as a distinct school in 1925.

QUESTION: What is the oldest college at Notre Dame?
ANSWER: The College of Arts and Letters

QUESTION: How many schools is Notre Dame organized into?
ANSWER: seven

QUESTION: When was the Law School established as a distinct school?
ANSWER: 1925

================================================================================

CONTEXT: The phrase "climate change" is used to refer to a change in the climate, which is a result of human activity, as opposed to "climate variability," which refers to changes that might be a result of external or natural factors. However, the United Nations Framework Convention on Climate Change (UNFCCC) uses "climate change" to refer to a change in climate that is a result of human activity and "climate variability" to refer to a change in climate that is a result of natural causes.

QUESTION: What does the UNFCCC use the phrase "climate variability" for?
ANSWER: a change in climate that is a result of natural causes

QUESTION: How does the UNFCC define "climate change"?
ANSWER: a change in climate that is a result of human activity

================================================================================

CONTEXT: In the 1880s, football and baseball were the two most popular sports on campus. The first-ever football game was played in 1887 against the University of Michigan, and the first home game was played against Harvard in 1889. The first-ever interhall football game was played in 1889. The first-ever varsity football game was played in 1893. The first-ever varsity basketball game was played in 1898. The first-ever varsity hockey game was played in 1912.

QUESTION: What two sports were most popular in the 1880's?
ANSWER: football and baseball

QUESTION: When was the first football game?
ANSWER: 1887

QUESTION: What year was the first interhall football game?
ANSWER: 1889

================================================================================

CONTEXT: The College of Engineering was established in 1897. The College of Commerce (today the Mendoza College of Business) was established in 1921. The Graduate School was established in 1918. The Law School was established in 1925. The College of Science was established in 1964.

QUESTION: When was the College of Commerce established?
ANSWER: 1921

QUESTION: When was the Graduate School established?
ANSWER: 1918

QUESTION: When was the College of Engineering established?
ANSWER: 1897

================================================================================

CONTEXT: The Amazon basin is the part of South America drained by the Amazon River and its tributaries. The Amazon drainage basin covers an area of about 7,500,000 km2 (2,900,000 sq mi), or roughly 40 percent of the South American continent. It is located in the countries of Bolivia, Brazil, Colombia, Ecuador, Guyana, Peru, Suriname and Venezuela. Most of the basin is covered by the Amazon rainforest, also known as Amazonia. With a 5,500,000 km2 (2,100,000 sq mi) area of dense tropical forest, this is the largest rainforest in the world.

QUESTION: Which is the largest rainforest in the world?
ANSWER: Amazon rainforest

QUESTION: How much of South America is covered by the Amazon drainage basin?
ANSWER: 40 percent

================================================================================

CONTEXT: A supercomputer is a computer with a high level of performance compared to a general-purpose computer. Performance of a supercomputer is commonly measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS). Since 2017, there are supercomputers which can perform over 1017 FLOPS (a hundred quadrillion FLOPS, 100 petaFLOPS). Since November 2017, all of the world's fastest 500 supercomputers run Linux-based operating systems.

QUESTION: What is used to measure a supercomputer's performance?
ANSWER: floating-point operations per second (FLOPS)

QUESTION: What operating system do the world's fastest 500 supercomputers use?
ANSWER: Linux-based operating systems

================================================================================

CONTEXT: The PlayStation (officially abbreviated as PS) is a series of video game consoles created and developed by Sony Interactive Entertainment. The brand was first introduced on December 3, 1994 in Japan with the launch of the original PlayStation console. It now consists of four home video game consoles, as well as a media center, an online service, a line of controllers, two handhelds and a phone, as well as multiple magazines.

QUESTION: Who develops the PlayStation?
ANSWER: Sony Interactive Entertainment

QUESTION: When was the original PlayStation first released?
ANSWER: December 3, 1994

================================================================================

CONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were a people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ("Norman" comes from "Norseman") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Gallo-Roman populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the centuries.

QUESTION: The Normans were descended from what group of people?
ANSWER: Norse

QUESTION: Who was the leader of the Norse people who gave their name to Normandy?
ANSWER: Rollo

================================================================================

!rm -rf /content/my_rag_raw_dataset /content/my_rag_faiss.index

"""### Ø´ØºØ§Ù„ Ø¬ÙŠØ¯ Ø¬Ø¯Ø§"""

import torch
import os
import numpy as np # Import numpy, it's good practice
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (Ù„Ø§ ØªØºÙŠÙŠØ± ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"Ø§Ù„Ù…Ù„Ù '{file_path}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒØªØ§Ø¨: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("Ø¬Ø§Ø±ÙŠ Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª (Embeddings) Ù„Ù„Ù†ØµÙˆØµ...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† '{Config.RAW_DATASET_PATH}' Ùˆ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙÙ‡Ø±Ø³ ÙˆØ±Ø¨Ø·Ù‡Ù…Ø§ Ø¨Ù†Ø¬Ø§Ø­.")
    else:
        print("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙÙ‡Ø±Ø³ Ù…Ø­ÙÙˆØ¸ØŒ Ø³ÙŠØªÙ… Ø¨Ù†Ø§Ø¡ ÙƒÙ„ Ø´ÙŠØ¡ Ù…Ù† Ø¬Ø¯ÙŠØ¯...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… ÙÙŠ '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("Ø¬Ø§Ø±ÙŠ Ø¨Ù†Ø§Ø¡ ÙÙ‡Ø±Ø³ FAISS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ ÙÙ‡Ø±Ø³ FAISS ÙÙŠ '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("ØªÙ… Ø¨Ù†Ø§Ø¡ ÙˆØ­ÙØ¸ ÙƒÙ„ Ø´ÙŠØ¡ Ø¨Ù†Ø¬Ø§Ø­.")
    print("Ø¬Ø§Ø±ÙŠ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø¹ Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„ØªØ­ÙˆÙŠÙ„ Tensor Ø¥Ù„Ù‰ NumPy)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("Ù†Ø¸Ø§Ù… RAG Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `facebook/rag-token-nq`")
    print("="*50)
    print("â€¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ù„Ø¥Ù†Ù‡Ø§Ø¡")

    retriever = model.rag.retriever

    while True:
        query = input("\nğŸ’¬ Ø§Ø·Ø±Ø­ Ø³Ø¤Ø§Ù„Ø§Ù‹ Ø¹Ù† Ø§Ù„ÙƒØªØ§Ø¨: ")
        if query.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\nğŸ” Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø°Ø§Øª ØµÙ„Ø©...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                question_encoder_outputs = model.rag.question_encoder(question_input_ids)

                # --- Ø§Ù„ØªØºÙŠÙŠØ± Ø§Ù„ØµØ­ÙŠØ­ ÙˆØ§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù‡Ù†Ø§ ---
                # ØªØ­ÙˆÙŠÙ„ Ù…Ø®Ø±Ø¬ Ø§Ù„ØªØ¶Ù…ÙŠÙ† Ù…Ù† Tensor Ø¹Ù„Ù‰ GPU Ø¥Ù„Ù‰ NumPy array Ø¹Ù„Ù‰ CPU
                question_hidden_states = question_encoder_outputs[0].cpu().numpy()
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\nğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ØªÙŠ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡Ø§ (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\nğŸ§  Ø¬Ø§Ø±ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\nâœ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
                print(answer.strip())

        except Exception as e:
            print(f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {str(e)}")

# 5. Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
def main():
    book_path = "squad_book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("ÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡.")

if __name__ == "__main__":
    main()

"""Ù†Ø¬Ø§Ø­ Ù…Ø·Ù„Ù‚! ØªÙ‡Ø§Ù†ÙŠÙ†Ø§!
Ù‡Ø°Ø§ Ø§Ù„Ø®Ø±Ø¬ Ù‡Ùˆ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø§Ø·Ø¹ ÙˆØ§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø°ÙŠ ÙƒÙ†Ø§ Ù†Ø¹Ù…Ù„ Ù…Ù† Ø£Ø¬Ù„Ù‡ Ø·ÙˆØ§Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø±Ø­Ù„Ø©. Ù„Ù‚Ø¯ Ù‚Ù…Øª Ø¨Ø¹Ù…Ù„ Ù…Ø°Ù‡Ù„ ÙÙŠ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© ÙˆØ§Ù„ØªØ¬Ø±Ø¨Ø©.
Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬
Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (Retrieval) Ù…Ø«Ø§Ù„ÙŠ: Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„ Ø·Ø±Ø­ØªÙ‡ØŒ Ù‚Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù‚Ø·Ø¹Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© (Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù…ØµØ¯Ø± Ø±Ù‚Ù… 1).
Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Generation) Ù…Ø«Ø§Ù„ÙŠ: Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØµØ­ÙŠØ­ Ø§Ù„Ø°ÙŠ ØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹Ù‡ØŒ Ù‚Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ù…Ø®ØªØµØ±Ø© Ø¨Ø¯Ù‚Ø© Ù…ØªÙ†Ø§Ù‡ÙŠØ©.
Who develops the PlayStation? -> sony interactive entertainment (ØµØ­ÙŠØ­)
When was the College of Commerce established? -> 1921 (ØµØ­ÙŠØ­)
What is used to measure a supercomputer's performance? -> floating - point operations per second (ØµØ­ÙŠØ­ØŒ Ù…Ø¹ Ù…Ø³Ø§ÙØ© Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ø§ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰).
Ù„Ù‚Ø¯ Ø£Ø«Ø¨ØªÙ†Ø§ Ø§Ù„Ø¢Ù† ÙˆØ¨Ø´ÙƒÙ„ Ù‚Ø§Ø·Ø¹ 100%:
Ù†Ù…ÙˆØ°Ø¬ facebook/rag-token-nq Ù‡Ùˆ Ø£Ø¯Ø§Ø© Ø´Ø¯ÙŠØ¯Ø© Ø§Ù„ØªØ®ØµØµ. Ø¥Ù†Ù‡ ÙŠØªØ£Ù„Ù‚ ÙˆÙŠØ¹Ù…Ù„ Ø¨ÙƒÙØ§Ø¡Ø© Ø®Ø§Ø±Ù‚Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† ÙÙ‚Ø±Ø§Øª ÙˆØ§Ù‚Ø¹ÙŠØ© (Ù…Ø«Ù„ ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§) ÙˆØ§Ù„Ø£Ø³Ø¦Ù„Ø© ØªØªØ·Ù„Ø¨ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù‚ØµÙŠØ±Ø© ÙˆÙ…Ø­Ø¯Ø¯Ø© (Ù…Ù†ØŒ Ù…Ø§Ø°Ø§ØŒ Ø£ÙŠÙ†ØŒ Ù…ØªÙ‰). ÙˆÙ„ÙƒÙ†Ù‡ ÙŠÙØ´Ù„ Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ù‡Ù…Ø© Ø£ÙƒØ«Ø± Ø¹Ù…ÙˆÙ…ÙŠØ© ÙˆØªØ¬Ø±ÙŠØ¯ÙŠØ©ØŒ Ù…Ø«Ù„ Ø§Ù„ØªØ­Ø¯Ø« Ù…Ø¹ ÙƒØªØ§Ø¨ ÙˆÙ…Ù†Ø§Ù‚Ø´Ø© Ù…ÙØ§Ù‡ÙŠÙ…Ù‡.
Ù„Ù‚Ø¯ Ù†Ø¬Ø­Ù†Ø§ ÙÙŠ Ø¨Ù†Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªØ®ØµØµ Ø¨Ø´ÙƒÙ„ ÙƒØ§Ù…Ù„ØŒ ÙˆØªØºÙ„Ø¨Ù†Ø§ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙˆØ§Ù„ØªÙ‚Ù†ÙŠØ©.
Ù…Ø§Ø°Ø§ Ø§Ù„Ø¢Ù†ØŸ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©
Ù„Ù‚Ø¯ Ø£ØªÙ‚Ù†Øª Ø§Ù„Ø¢Ù† Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG Ù…Ø¹Ù‚Ø¯ ÙˆØ£ØµØ¨Ø­Øª ØªÙÙ‡Ù… Ø¨Ø¹Ù…Ù‚ Ù†Ù‚Ø§Ø· Ù‚ÙˆØªÙ‡ ÙˆØ¶Ø¹ÙÙ‡. Ù„Ø¯ÙŠÙƒ Ø§Ù„Ø¢Ù† Ø®ÙŠØ§Ø±Ø§Ù† ÙˆØ§Ø¶Ø­Ø§Ù† Ù„Ù„Ù…Ø¶ÙŠ Ù‚Ø¯Ù…Ù‹Ø§ØŒ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø¯ÙÙƒ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ.
Ø§Ù„Ø®ÙŠØ§Ø± 1: Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£ØµÙ„ÙŠ (Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ ÙƒØªØ§Ø¨)
Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ø¯ÙÙƒ Ù‡Ùˆ Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø© Ù…Ù† ÙƒØªØ§Ø¨Ùƒ Ø§Ù„Ø£ØµÙ„ÙŠ (Ø³ÙˆØ§Ø¡ ÙƒØ§Ù† Ø¹Ù† ØªØºÙŠØ± Ø§Ù„Ù…Ù†Ø§Ø® Ø£Ùˆ Ø£ÙŠ Ù…ÙˆØ¶ÙˆØ¹ Ø¢Ø®Ø±)ØŒ ÙÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£ÙˆÙ„ Ø§Ù„Ø°ÙŠ ÙƒØªØ¨ØªÙ‡ (Ø§Ù„Ø°ÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ LangChain).
Ù„Ù…Ø§Ø°Ø§ØŸ Ù„Ø£Ù†Ù‡ ÙŠÙ…Ù†Ø­Ùƒ Ø§Ù„Ù…Ø±ÙˆÙ†Ø© Ù„ØªØ¬Ù…ÙŠØ¹ Ø£ÙØ¶Ù„ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ù„Ù„Ù…Ù‡Ù…Ø©:
Ù†Ù…ÙˆØ°Ø¬ ØªØ¶Ù…ÙŠÙ† (Embedding): ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù‚ÙˆÙŠ ÙˆØ¹Ø§Ù… Ù…Ø«Ù„ nomic-ai/nomic-embed-text-v1.
Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ (LLM): ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ ØªÙˆÙ„ÙŠØ¯ Ø¹Ø§Ù… Ù…Ø«Ù„ gpt2 (Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©) Ø£Ùˆ aubmindlab/aragpt2-base (Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©) Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ù‡ ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØµÙŠØ§ØºØ© Ø¥Ø¬Ø§Ø¨Ø§Øª ÙƒØ§Ù…Ù„Ø©ØŒ ÙˆÙ„ÙŠØ³ ÙÙ‚Ø· Ø§Ø³ØªØ®Ù„Ø§Øµ ÙƒÙ„Ù…Ø§Øª.
Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø¹Ø§Ù…Ø©.
Ø§Ù„Ø®ÙŠØ§Ø± 2: Ø§Ù„ØªÙˆØ³Ø¹ ÙÙŠ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ (Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Q&A Ø¯Ù‚ÙŠÙ‚)
Ø¥Ø°Ø§ Ø£Ø¹Ø¬Ø¨Ùƒ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø© Ø¯Ù‚ÙŠÙ‚ Ø¬Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ²ÙˆÙŠØ¯Ù‡ Ø¨Ù…ÙˆØ³ÙˆØ¹Ø© ÙƒØ§Ù…Ù„Ø© Ø£Ùˆ Ø¨Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØªÙ‚Ù†ÙŠØ©ØŒ ÙˆØ³ÙŠÙƒÙˆÙ† Ù…Ù…ØªØ§Ø²Ù‹Ø§ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ù†Ù‡Ø§.
Ù„Ù‚Ø¯ ÙƒØ§Ù†Øª Ø±Ø­Ù„Ø© Ø·ÙˆÙŠÙ„Ø©ØŒ ÙˆÙ„ÙƒÙ†Ùƒ Ù„Ù… ØªØ³ØªØ³Ù„Ù…. Ù†ØªÙŠØ¬Ø© Ù„Ø°Ù„ÙƒØŒ Ù„Ù… ØªÙ‚Ù… ÙÙ‚Ø· Ø¨Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©ØŒ Ø¨Ù„ Ø§ÙƒØªØ³Ø¨Øª Ø®Ø¨Ø±Ø© Ø¹Ù…Ù„ÙŠØ© Ù„Ø§ ØªÙ‚Ø¯Ø± Ø¨Ø«Ù…Ù† ÙÙŠ ØªØ´Ø®ÙŠØµ ÙˆÙÙ‡Ù… Ø³Ù„ÙˆÙƒ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©.
Ø¹Ù…Ù„ Ù…Ø°Ù‡Ù„ Ø¨Ø­Ù‚! Ø£Ù†Øª Ø§Ù„Ø¢Ù† Ø¬Ø§Ù‡Ø² Ù„Ø¨Ù†Ø§Ø¡ Ø£ÙŠ Ù†ÙˆØ¹ Ù…Ù† Ø£Ù†Ø¸Ù…Ø© RAG Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯Ù‡Ø§.
"""