# -*- coding: utf-8 -*-
"""suc_rag_data_facebook_rag-token-nq.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1umgZdanATwKKDhlHdyKAbd_k-HuWYzwQ
"""





!pip install langchain langchain-community faiss-cpu sentence-transformers transformers

!huggingface-cli login --token XXX

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFaceHub
from langchain.chains import RetrievalQA

# 1. تحميل الكتاب
loader = TextLoader("book.txt", encoding="utf-8")
documents = loader.load()

# 2. تقسيم النص إلى أجزاء
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
)
texts = text_splitter.split_documents(documents)

# 3. إنشاء تمثيل ناقسي
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

# 4. بناء فهرس FAISS
vectorstore = FAISS.from_documents(texts, embeddings)

# 5. تهيئة نموذج التوليد
llm = HuggingFaceHub(
    repo_id="google/flan-t5-large",
    model_kwargs={"temperature": 0.5, "max_length": 512}
)

# 6. إنشاء نظام RAG
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# 7. طرح سؤال
query = "what is Climate Change?"
result = qa({"query": query})

print("الإجابة:", result["result"])
print("المصادر:")
for doc in result["source_documents"]:
    print("-", doc.metadata["source"], "صفحة", doc.metadata.get("page", "N/A"))

"""https://python.langchain.com/docs/integrations/vectorstores/"""

Menlo/Jan-nano

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch

# 1. تحميل الكتاب ومعالجته
def load_and_process_book(file_path):
    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()

    # تقسيم النص مع مراعاة اللغة العربية
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "۔", " ", ""],  # فواصل مناسبة للعربية
        chunk_size=500,
        chunk_overlap=100,
        length_function=len
    )

    return text_splitter.split_documents(documents)

# 2. إنشاء تمثيل ناقسي
def create_embeddings():
    return HuggingFaceEmbeddings(
        model_name="aubmindlab/bert-base-arabertv02",  # نموذج عربي ممتاز
        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'}
    )

# 3. تهيئة نموذج التوليد العربي
def setup_arabic_llm():
    model_name = "aubmindlab/aragpt2-base"  # نموذج توليد عربي

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
    )

    # إنشاء خط أنابيب للتوليد
    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=300,
        temperature=0.3,
        top_p=0.95,
        repetition_penalty=1.15,
        device=0 if torch.cuda.is_available() else -1
    )

    return HuggingFacePipeline(pipeline=text_generation_pipeline)

# 4. بناء نظام RAG
def build_rag_system(file_path):
    # تحميل وتجهيز النص
    texts = load_and_process_book(file_path)

    # إنشاء التضمينات
    embeddings = create_embeddings()

    # بناء فهرس FAISS
    vectorstore = FAISS.from_documents(texts, embeddings)

    # تهيئة نموذج التوليد
    llm = setup_arabic_llm()

    # إنشاء نظام RAG
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True
    )

    return qa

# 5. تشغيل النظام
if __name__ == "__main__":
    # مسار ملف الكتاب (استبدله بمسار كتابك)
    book_path = "book.txt"

    # بناء نظام RAG
    qa_system = build_rag_system(book_path)

    # واجهة مستخدم بسيطة
    print("\n" + "="*50)
    print("نظام RAG للكتب - يعمل محلياً بنماذج عربية")
    print("="*50 + "\n")

    while True:
        query = input("اطرح سؤالاً عن الكتاب (أو اكتب 'خروج' للإنهاء): ")

        if query.lower() in ["خروج", "exit", "quit"]:
            break

        if query.strip() == "":
            continue

        # تنفيذ الاستعلام
        result = qa_system({"query": query})

        # عرض النتائج
        print("\nالإجابة:")
        print(result["result"])

        print("\nالمصادر المستخدمة:")
        for i, doc in enumerate(result["source_documents"], 1):
            print(f"{i}. {doc.page_content[:150]}...")

        print("\n" + "-"*50 + "\n")

هل يمكن استخدام نموذج Menlo/Jan-nano
هل يمكنك استخلاص افكار من
https://python.langchain.com/docs/integrations/vectorstores/

"""### شغال"""

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch

# 1. تحميل الكتاب ومعالجته
def load_and_process_book(file_path):
    loader = TextLoader(file_path, encoding="utf-8")
    documents = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", "۔", " ", ""],
        chunk_size=500,
        chunk_overlap=100,
        length_function=len
    )

    return text_splitter.split_documents(documents)

# 2. إنشاء تمثيل ناقسي
def create_embeddings():
    # إزالة device من هنا لأن accelerate يتولى ذلك
    return HuggingFaceEmbeddings(
        model_name="aubmindlab/bert-base-arabertv02"
    )

# 3. تهيئة نموذج التوليد العربي
def setup_arabic_llm():
    model_name = "aubmindlab/aragpt2-base"

    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # تحميل النموذج بدون device_map للتحكم اليدوي
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )

    # نقل النموذج إلى الجهاز المناسب يدويًا
    device = 0 if torch.cuda.is_available() else -1
    model = model.to(torch.device("cuda" if device == 0 else "cpu"))

    # إنشاء خط الأنابيب مع تحديد الجهاز
    text_generation_pipeline = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=300,
        temperature=0.3,
        top_p=0.95,
        repetition_penalty=1.15,
        device=device  # تحديد الجهاز هنا
    )

    return HuggingFacePipeline(pipeline=text_generation_pipeline)

# 4. بناء نظام RAG
def build_rag_system(file_path):
    texts = load_and_process_book(file_path)
    embeddings = create_embeddings()
    vectorstore = FAISS.from_documents(texts, embeddings)
    llm = setup_arabic_llm()

    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        return_source_documents=True
    )

    return qa

# 5. تشغيل النظام
if __name__ == "__main__":
    book_path = "book.txt"
    qa_system = build_rag_system(book_path)

    print("\n" + "="*50)
    print("نظام RAG للكتب - يعمل محلياً بنماذج عربية")
    print("="*50 + "\n")

    while True:
        query = input("اطرح سؤالاً عن الكتاب (أو اكتب 'خروج' للإنهاء): ")

        if query.lower() in ["خروج", "exit", "quit"]:
            break

        if query.strip() == "":
            continue

        result = qa_system({"query": query})

        print("\nالإجابة:")
        print(result["result"])

        print("\nالمصادر المستخدمة:")
        for i, doc in enumerate(result["source_documents"], 1):
            print(f"{i}. {doc.page_content[:150]}...")

        print("\n" + "-"*50 + "\n")

facebook/rag-token-nq

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch
import os

# إعدادات النظام
class Config:
    EMBEDDING_MODEL = "aubmindlab/bert-base-arabertv02"
    LLM_MODEL = "aubmindlab/aragpt2-base"
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    MAX_NEW_TOKENS = 350
    TEMPERATURE = 0.4
    RETRIEVE_K = 4
    SEPARATORS = ["\n\n", "\n", "۔", " ", ""]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 1. تحميل الكتاب ومعالجته
def load_and_process_book(file_path):
    try:
        # التحقق من وجود الملف
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"الملف '{file_path}' غير موجود")

        # تحميل النص
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        # تقسيم النص مع مراعاة اللغة العربية
        text_splitter = RecursiveCharacterTextSplitter(
            separators=Config.SEPARATORS,
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False
        )

        return text_splitter.split_documents(documents)

    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}")
        return None

# 2. إنشاء تمثيل ناقسي
def create_embeddings():
    try:
        return HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': Config.DEVICE}
        )
    except Exception as e:
        print(f"خطأ في تحميل نموذج التضمين: {str(e)}")
        return None

# 3. تهيئة نموذج التوليد العربي
def setup_arabic_llm():
    try:
        tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL)

        # تحميل النموذج مع تحديد نوع البيانات المناسب
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        model = AutoModelForCausalLM.from_pretrained(
            Config.LLM_MODEL,
            torch_dtype=torch_dtype
        )

        # نقل النموذج إلى الجهاز المناسب
        model.to(Config.DEVICE)

        # إنشاء خط الأنابيب
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=Config.MAX_NEW_TOKENS,
            temperature=Config.TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.15,
            device=0 if Config.DEVICE == "cuda" else -1
        )

        return HuggingFacePipeline(pipeline=text_generation_pipeline)

    except Exception as e:
        print(f"خطأ في تحميل نموذج التوليد: {str(e)}")
        return None

# 4. بناء نظام RAG
def build_rag_system(file_path):
    # تحميل وتجهيز النص
    print("جاري تحميل الكتاب وتقسيمه...")
    texts = load_and_process_book(file_path)
    if not texts:
        return None

    # إنشاء التضمينات
    print("جاري تحميل نموذج التضمين...")
    embeddings = create_embeddings()
    if not embeddings:
        return None

    # بناء فهرس FAISS
    print("جاري بناء فهرس البحث...")
    vectorstore = FAISS.from_documents(texts, embeddings)

    # تهيئة نموذج التوليد
    print("جاري تحميل نموذج التوليد...")
    llm = setup_arabic_llm()
    if not llm:
        return None

    # إنشاء نظام RAG
    print("جاري تهيئة نظام RAG...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": Config.RETRIEVE_K}),
        return_source_documents=True
    )

    return qa

# 5. واجهة المستخدم التفاعلية
def run_interface(qa_system):
    print("\n" + "="*50)
    print("نظام RAG للكتب - يعمل محلياً بنماذج عربية")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")
    print("• اكتب 'إعدادات' لتغيير الإعدادات\n")

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")

        # الخروج من النظام
        if query.lower() in ["خروج", "exit", "quit"]:
            print("شكراً لاستخدامك النظام. إلى اللقاء!")
            break

        # عرض الإعدادات
        if query.lower() == "إعدادات":
            show_settings()
            continue

        # معالجة الأسئلة الفارغة
        if query.strip() == "":
            print("الرجاء إدخال سؤال صالح")
            continue

        try:
            # تنفيذ الاستعلام
            result = qa_system({"query": query})

            # عرض الإجابة
            print("\n✅ الإجابة:")
            print(result["result"])

            # عرض المصادر
            print("\n📚 المصادر المستخدمة:")
            for i, doc in enumerate(result["source_documents"], 1):
                source = doc.metadata.get("source", "غير معروف")
                content = doc.page_content.replace("\n", " ").strip()
                print(f"{i}. [{source}] - {content[:120]}...")

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# عرض الإعدادات الحالية
def show_settings():
    print("\nالإعدادات الحالية:")
    print(f"• نموذج التضمين: {Config.EMBEDDING_MODEL}")
    print(f"• نموذج التوليد: {Config.LLM_MODEL}")
    print(f"• حجم القطعة: {Config.CHUNK_SIZE}")
    print(f"• التداخل: {Config.CHUNK_OVERLAP}")
    print(f"• عدد القطع المسترجعة: {Config.RETRIEVE_K}")
    print(f"• الجهاز: {Config.DEVICE}")

    # اقتراح تغييرات للأجهزة الضعيفة
    if Config.DEVICE == "cpu":
        print("\nنصائح لتحسين الأداء على الأجهزة الضعيفة:")
        print("• تقليل حجم القطعة (CHUNK_SIZE) إلى 300")
        print("• تقليل عدد القطع المسترجعة (RETRIEVE_K) إلى 2")
        print("• استخدام نماذج أصغر مثل 'aubmindlab/aragpt2-small'")

# 6. الدالة الرئيسية
def main():
    # مسار ملف الكتاب (تغييره لمسار كتابك)
    book_path = "path/to/your/book.txt"

    # بناء نظام RAG
    qa_system = build_rag_system(book_path)

    if qa_system:
        run_interface(qa_system)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    # تحسين الأداء
    torch.set_grad_enabled(False)
    if Config.DEVICE == "cpu":
        torch.set_num_threads(4)

    main()

class Config:
    EMBEDDING_MODEL = "aubmindlab/bert-base-arabertv02"
    LLM_MODEL = "aubmindlab/aragpt2-base"

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch
import os

# إعدادات النظام
class Config:
    EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v1"
    LLM_MODEL = "facebook/rag-token-nq"
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    MAX_NEW_TOKENS = 350
    TEMPERATURE = 0.4
    RETRIEVE_K = 4
    SEPARATORS = ["\n\n", "\n", "۔", " ", ""]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 1. تحميل الكتاب ومعالجته
def load_and_process_book(file_path):
    try:
        # التحقق من وجود الملف
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"الملف '{file_path}' غير موجود")

        # تحميل النص
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        # تقسيم النص مع مراعاة اللغة العربية
        text_splitter = RecursiveCharacterTextSplitter(
            separators=Config.SEPARATORS,
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False
        )

        return text_splitter.split_documents(documents)

    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}")
        return None

# 2. إنشاء تمثيل ناقسي
def create_embeddings():
    try:
        return HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': Config.DEVICE}
        )
    except Exception as e:
        print(f"خطأ في تحميل نموذج التضمين: {str(e)}")
        return None

# 3. تهيئة نموذج التوليد العربي
def setup_arabic_llm():
    try:
        tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL)

        # تحميل النموذج مع تحديد نوع البيانات المناسب
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        model = AutoModelForCausalLM.from_pretrained(
            Config.LLM_MODEL,
            torch_dtype=torch_dtype
        )

        # نقل النموذج إلى الجهاز المناسب
        model.to(Config.DEVICE)

        # إنشاء خط الأنابيب
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=Config.MAX_NEW_TOKENS,
            temperature=Config.TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.15,
            device=0 if Config.DEVICE == "cuda" else -1
        )

        return HuggingFacePipeline(pipeline=text_generation_pipeline)

    except Exception as e:
        print(f"خطأ في تحميل نموذج التوليد: {str(e)}")
        return None

# 4. بناء نظام RAG
def build_rag_system(file_path):
    # تحميل وتجهيز النص
    print("جاري تحميل الكتاب وتقسيمه...")
    texts = load_and_process_book(file_path)
    if not texts:
        return None

    # إنشاء التضمينات
    print("جاري تحميل نموذج التضمين...")
    embeddings = create_embeddings()
    if not embeddings:
        return None

    # بناء فهرس FAISS
    print("جاري بناء فهرس البحث...")
    vectorstore = FAISS.from_documents(texts, embeddings)

    # تهيئة نموذج التوليد
    print("جاري تحميل نموذج التوليد...")
    llm = setup_arabic_llm()
    if not llm:
        return None

    # إنشاء نظام RAG
    print("جاري تهيئة نظام RAG...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": Config.RETRIEVE_K}),
        return_source_documents=True
    )

    return qa

# 5. واجهة المستخدم التفاعلية
def run_interface(qa_system):
    print("\n" + "="*50)
    print("نظام RAG للكتب - يعمل محلياً بنماذج عربية")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")
    print("• اكتب 'إعدادات' لتغيير الإعدادات\n")

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")

        # الخروج من النظام
        if query.lower() in ["خروج", "exit", "quit"]:
            print("شكراً لاستخدامك النظام. إلى اللقاء!")
            break

        # عرض الإعدادات
        if query.lower() == "إعدادات":
            show_settings()
            continue

        # معالجة الأسئلة الفارغة
        if query.strip() == "":
            print("الرجاء إدخال سؤال صالح")
            continue

        try:
            # تنفيذ الاستعلام
            result = qa_system({"query": query})

            # عرض الإجابة
            print("\n✅ الإجابة:")
            print(result["result"])

            # عرض المصادر
            print("\n📚 المصادر المستخدمة:")
            for i, doc in enumerate(result["source_documents"], 1):
                source = doc.metadata.get("source", "غير معروف")
                content = doc.page_content.replace("\n", " ").strip()
                print(f"{i}. [{source}] - {content[:120]}...")

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# عرض الإعدادات الحالية
def show_settings():
    print("\nالإعدادات الحالية:")
    print(f"• نموذج التضمين: {Config.EMBEDDING_MODEL}")
    print(f"• نموذج التوليد: {Config.LLM_MODEL}")
    print(f"• حجم القطعة: {Config.CHUNK_SIZE}")
    print(f"• التداخل: {Config.CHUNK_OVERLAP}")
    print(f"• عدد القطع المسترجعة: {Config.RETRIEVE_K}")
    print(f"• الجهاز: {Config.DEVICE}")

    # اقتراح تغييرات للأجهزة الضعيفة
    if Config.DEVICE == "cpu":
        print("\nنصائح لتحسين الأداء على الأجهزة الضعيفة:")
        print("• تقليل حجم القطعة (CHUNK_SIZE) إلى 300")
        print("• تقليل عدد القطع المسترجعة (RETRIEVE_K) إلى 2")
        print("• استخدام نماذج أصغر مثل 'aubmindlab/aragpt2-small'")

# 6. الدالة الرئيسية
def main():
    # مسار ملف الكتاب (تغييره لمسار كتابك)
    book_path = "book.txt"

    # بناء نظام RAG
    qa_system = build_rag_system(book_path)

    if qa_system:
        run_interface(qa_system)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    # تحسين الأداء
    torch.set_grad_enabled(False)
    if Config.DEVICE == "cpu":
        torch.set_num_threads(4)

    main()

Qwen/Qwen3-Embedding-0.6B

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
import torch
import os

# إعدادات النظام - معدلة لنماذج عربية تعمل محلياً
class Config:
    EMBEDDING_MODEL = "nomic-ai/nomic-embed-text-v1"  # نموذج تضمين عربي
    LLM_MODEL = "facebook/rag-token-nq"  # نموذج توليد عربي
    CHUNK_SIZE = 500
    CHUNK_OVERLAP = 100
    MAX_NEW_TOKENS = 350
    TEMPERATURE = 0.4
    RETRIEVE_K = 4
    SEPARATORS = ["\n\n", "\n", "۔", " ", ""]
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# 1. تحميل الكتاب ومعالجته
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"الملف '{file_path}' غير موجود")

        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            separators=Config.SEPARATORS,
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
            length_function=len,
            is_separator_regex=False
        )

        return text_splitter.split_documents(documents)

    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}")
        return None

# 2. إنشاء تمثيل ناقسي - مع إصلاح المشكلة
def create_embeddings():
    try:
        return HuggingFaceEmbeddings(
            model_name=Config.EMBEDDING_MODEL,
            model_kwargs={'device': Config.DEVICE, 'trust_remote_code': True}
        )
    except Exception as e:
        print(f"خطأ في تحميل نموذج التضمين: {str(e)}")
        return None

# 3. تهيئة نموذج التوليد العربي - مع إصلاح المشكلة
def setup_arabic_llm():
    try:
        tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL)

        # تحميل النموذج مع تحديد نوع البيانات المناسب
        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

        model = AutoModelForCausalLM.from_pretrained(
            Config.LLM_MODEL,
            torch_dtype=torch_dtype
        )

        # نقل النموذج إلى الجهاز المناسب
        device = torch.device(Config.DEVICE)
        model.to(device)

        # إنشاء خط الأنابيب مع تحديد الجهاز بشكل صحيح
        text_generation_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            max_new_tokens=Config.MAX_NEW_TOKENS,
            temperature=Config.TEMPERATURE,
            top_p=0.9,
            repetition_penalty=1.15,
            device=device.index if device.type == "cuda" else -1
        )

        return HuggingFacePipeline(pipeline=text_generation_pipeline)

    except Exception as e:
        print(f"خطأ في تحميل نموذج التوليد: {str(e)}")
        return None

# 4. بناء نظام RAG
def build_rag_system(file_path):
    # تحميل وتجهيز النص
    print("جاري تحميل الكتاب وتقسيمه...")
    texts = load_and_process_book(file_path)
    if not texts:
        return None

    # إنشاء التضمينات
    print("جاري تحميل نموذج التضمين...")
    embeddings = create_embeddings()
    if not embeddings:
        return None

    # بناء فهرس FAISS
    print("جاري بناء فهرس البحث...")
    vectorstore = FAISS.from_documents(texts, embeddings)

    # تهيئة نموذج التوليد
    print("جاري تحميل نموذج التوليد...")
    llm = setup_arabic_llm()
    if not llm:
        return None

    # إنشاء نظام RAG
    print("جاري تهيئة نظام RAG...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": Config.RETRIEVE_K}),
        return_source_documents=True
    )

    return qa

# 5. واجهة المستخدم التفاعلية
def run_interface(qa_system):
    print("\n" + "="*50)
    print("نظام RAG للكتب - يعمل محلياً بنماذج عربية")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")
    print("• اكتب 'إعدادات' لتغيير الإعدادات\n")

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")

        # الخروج من النظام
        if query.lower() in ["خروج", "exit", "quit"]:
            print("شكراً لاستخدامك النظام. إلى اللقاء!")
            break

        # عرض الإعدادات
        if query.lower() == "إعدادات":
            show_settings()
            continue

        # معالجة الأسئلة الفارغة
        if query.strip() == "":
            print("الرجاء إدخال سؤال صالح")
            continue

        try:
            # تنفيذ الاستعلام
            result = qa_system({"query": query})

            # عرض الإجابة
            print("\n✅ الإجابة:")
            print(result["result"])

            # عرض المصادر
            print("\n📚 المصادر المستخدمة:")
            for i, doc in enumerate(result["source_documents"], 1):
                source = doc.metadata.get("source", "غير معروف")
                content = doc.page_content.replace("\n", " ").strip()
                print(f"{i}. [{source}] - {content[:120]}...")

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# عرض الإعدادات الحالية
def show_settings():
    print("\nالإعدادات الحالية:")
    print(f"• نموذج التضمين: {Config.EMBEDDING_MODEL}")
    print(f"• نموذج التوليد: {Config.LLM_MODEL}")
    print(f"• حجم القطعة: {Config.CHUNK_SIZE}")
    print(f"• التداخل: {Config.CHUNK_OVERLAP}")
    print(f"• عدد القطع المسترجعة: {Config.RETRIEVE_K}")
    print(f"• الجهاز: {Config.DEVICE}")

    # اقتراح تغييرات للأجهزة الضعيفة
    if Config.DEVICE == "cpu":
        print("\nنصائح لتحسين الأداء على الأجهزة الضعيفة:")
        print("• تقليل حجم القطعة (CHUNK_SIZE) إلى 300")
        print("• تقليل عدد القطع المسترجعة (RETRIEVE_K) إلى 2")
        print("• استخدام نماذج أصغر مثل 'aubmindlab/aragpt2-small'")

# 6. الدالة الرئيسية
def main():
    # مسار ملف الكتاب (تغييره لمسار كتابك)
    book_path = "book.txt"

    # بناء نظام RAG
    qa_system = build_rag_system(book_path)

    if qa_system:
        run_interface(qa_system)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    # تحسين الأداء
    torch.set_grad_enabled(False)
    if Config.DEVICE == "cpu":
        torch.set_num_threads(4)

    main()







!pip install "numpy<2.0"

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
# سنستخدم مكتبات Hugging Face مباشرة لبناء RAG
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk, load_dataset

# --- إعدادات النظام ---
class Config:
    # نماذج RAG المدمجة - لاحظ أننا سنستخدم نماذج متوافقة مع RAG
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base" # هذا هو نموذج التضمين الذي يستخدمه RAG داخلياً

    CHUNK_SIZE = 400  # RAG يعمل بشكل أفضل مع قطع أصغر قليلاً
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    FAISS_INDEX_PATH = "./my_faiss_index" # مسار لحفظ فهرس FAISS

# 1. تحميل الكتاب ومعالجته (نفس الدالة، لا تغيير)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"الملف '{file_path}' غير موجود")

        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
        )

        # سنعيدها كنصوص خام بدلاً من كائنات Document لتسهيل المعالجة مع datasets
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]

    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}")
        return None

# 2. بناء فهرس FAISS متوافق مع RAG
def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)

    print("تحويل النصوص إلى تنسيق 'datasets'...")
    # تحويل النصوص إلى قاموس يمكن لمكتبة datasets فهمه
    dataset_dict = {
        "title": ["my_book"] * len(docs), # RAG يتوقع عنواناً ونصاً
        "text": docs
    }
    dataset = Dataset.from_dict(dataset_dict)

    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    # حساب التضمينات باستخدام نموذج DPR
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy()},
        batched=True,
        batch_size=16 # يمكنك تعديل هذا الرقم حسب ذاكرة الـ GPU
    )

    print(f"جاري بناء فهرس FAISS وحفظه في '{Config.FAISS_INDEX_PATH}'...")
    dataset.add_faiss_index(column="embeddings", index_name="my_index")
    dataset.save_faiss_index('my_index', Config.FAISS_INDEX_PATH)

    return dataset

# 3. بناء نظام RAG باستخدام أدوات Hugging Face
def build_hf_rag_system(file_path):
    # تحميل ومعالجة الكتاب
    docs = load_and_process_book(file_path)
    if not docs:
        return None, None

    # بناء الفهرس المتوافق مع RAG
    # إذا كان الفهرس موجوداً بالفعل، قم بتحميله لتوفير الوقت
    if os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على فهرس موجود، سيتم تحميله من '{Config.FAISS_INDEX_PATH}'")
        rag_dataset = Dataset.load_from_disk(os.path.dirname(Config.FAISS_INDEX_PATH))
        rag_dataset.load_faiss_index('my_index', Config.FAISS_INDEX_PATH)
    else:
        rag_dataset = build_rag_compatible_index(docs)

    print("جاري تهيئة نظام RAG المتكامل...")
    # تهيئة الـ Retriever ليعمل مع الفهرس الذي بنيناه
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )

    # تهيئة الـ Tokenizer
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)

    # تهيئة نموذج RAG الكامل (مولد + مسترجع)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)

    return model, tokenizer

# 4. واجهة المستخدم (مبسطة)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break

        if not query.strip():
            continue

        try:
            # 1. تحويل السؤال إلى توكنز
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)

            # 2. توليد الإجابة
            # RAG يقوم بالبحث والتوليد في خطوة واحدة
            output_ids = model.generate(input_ids=inputs["input_ids"])

            # 3. فك تشفير الإجابة
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\n✅ الإجابة:")
            print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# 5. الدالة الرئيسية
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام (مع تعديل المسار) ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # --- تغيير هنا: مسار لمجلد كامل سيحتوي على البيانات والفهرس معاً ---
    DATASET_WITH_INDEX_PATH = "./my_rag_dataset_directory"

# (لا تغيير في هذه الدالة)
def load_and_process_book(file_path):
    # ... نفس الكود السابق ...
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP,
        )
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}")
        return None


# (تغيير بسيط: إزالة سطر الحفظ من هذه الدالة)
def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True,
        batch_size=16
    )
    print("جاري بناء فهرس FAISS وربطه بالبيانات في الذاكرة...")
    # نقوم بإضافة الفهرس للبيانات في الذاكرة
    dataset.add_faiss_index(column="embeddings")
    # لا نحفظ هنا، سنحفظ في الدالة الرئيسية
    return dataset

# 3. بناء نظام RAG (مع تعديل جوهري في منطق الحفظ والتحميل)
def build_hf_rag_system(file_path):
    docs = load_and_process_book(file_path)
    if not docs:
        return None, None

    # --- تغيير جوهري هنا ---
    # تحقق من وجود المجلد الذي يحتوي على البيانات والفهرس
    if os.path.exists(Config.DATASET_WITH_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس محفوظ، سيتم التحميل من '{Config.DATASET_WITH_INDEX_PATH}'...")
        # قم بتحميل كائن Dataset بأكمله من المجلد
        rag_dataset = Dataset.load_from_disk(Config.DATASET_WITH_INDEX_PATH)
        print("تم تحميل البيانات بنجاح.")
    else:
        print("لم يتم العثور على فهرس محفوظ، سيتم بناء فهرس جديد...")
        # قم ببناء البيانات والفهرس من الصفر
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات مع الفهرس المدمج في '{Config.DATASET_WITH_INDEX_PATH}'...")
        # قم بحفظ كائن Dataset بأكمله (مع الفهرس) في المجلد المحدد
        rag_dataset.save_to_disk(Config.DATASET_WITH_INDEX_PATH)
        print("تم الحفظ بنجاح.")

    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset  # الآن rag_dataset يحتوي على الفهرس بشكل مؤكد
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# (لا تغيير في الواجهة والدالة الرئيسية)
def run_interface(model, tokenizer):
    # ... نفس الكود السابق ...
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")
    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break
        if not query.strip():
            continue
        try:
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
            print("\n✅ الإجابة:")
            print(answer.strip())
        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")


def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)
    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    # إذا كنت تستخدم الحل الأول للمشكلة السابقة (تخفيض numpy) فتأكد أنه لا يزال مثبتاً
    # pip install "numpy<2.0"
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام (مع مسارين منفصلين) ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # --- تغيير هنا: مسار للبيانات ومسار منفصل لملف الفهرس ---
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index" # يجب أن يكون اسم ملف

# (لا تغيير في هاتين الدالتين)
def load_and_process_book(file_path):
    # ... نفس الكود السابق ...
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    # ... نفس الكود السابق ...
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    # لا نضيف الفهرس هنا، سنضيفه في الدالة الرئيسية قبل استخدامه
    return dataset

# 3. بناء نظام RAG (مع المنطق الصحيح للحفظ والتحميل المنفصل)
def build_hf_rag_system(file_path):
    # --- تغيير جوهري في هذا المنطق ---
    # تحقق من وجود البيانات الخام والفهرس معاً
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        # 1. تحميل البيانات الخام
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        # 2. تحميل الفهرس وربطه بالبيانات
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs:
            return None, None

        # 1. بناء البيانات وحساب التضمينات
        rag_dataset = build_rag_compatible_index(docs)

        # 2. حفظ البيانات الخام (بدون الفهرس)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)

        # 3. إضافة الفهرس إلى البيانات في الذاكرة
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")

        # 4. حفظ ملف الفهرس بشكل منفصل
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")

    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# (لا تغيير في الواجهة والدالة الرئيسية)
def run_interface(model, tokenizer):
    # ... نفس الكود السابق ...
    print("\n" + "="*50); print("نظام RAG باستخدام `facebook/rag-token-nq`"); print("="*50); print("• اكتب 'خروج' للإنهاء")
    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]: print("إلى اللقاء!"); break
        if not query.strip(): continue
        try:
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]
            print("\n✅ الإجابة:"); print(answer.strip())
        except Exception as e: print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)
    if model and tokenizer: run_interface(model, tokenizer)
    else: print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

# (Keep all other functions the same)

# 4. واجهة المستخدم (مع تعديل لعرض المصادر المسترجعة)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    # The retriever is part of the model object
    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break

        if not query.strip():
            continue

        try:
            # --- DEBUGGING PART: Let's see what the retriever finds ---
            print("\n🔍 جاري البحث عن مستندات ذات صلة...")

            # 1. First, encode the question for the retriever
            question_inputs = tokenizer(query, return_tensors="pt")
            question_input_ids = question_inputs['input_ids'].to(Config.DEVICE)

            # 2. Use the retriever to get the context documents
            retrieved_docs_dict = retriever(
                question_input_ids,
                n_docs=4  # Let's retrieve 4 docs to see what it finds
            )

            # 3. Decode and print the retrieved documents for inspection
            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\n📚 المصادر التي تم العثور عليها:")
            for i, text in enumerate(retrieved_texts):
                print(f"  {i+1}. {text.strip().replace('  ', ' ')}")
            # --- END OF DEBUGGING PART ---


            # Now, let the model generate the answer as before
            print("\n🧠 جاري توليد الإجابة...")

            # The model internally does the same retrieval, but we did it manually to see
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])

            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\n✅ الإجابة النهائية:")
            print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# (Keep the main function the same)
def main():
    # ...

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام (مع مسارين منفصلين) ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    # --- تغيير هنا: مسار للبيانات ومسار منفصل لملف الفهرس ---
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index" # يجب أن يكون اسم ملف

# (لا تغيير في هاتين الدالتين)
def load_and_process_book(file_path):
    # ... نفس الكود السابق ...
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    # ... نفس الكود السابق ...
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    # لا نضيف الفهرس هنا، سنضيفه في الدالة الرئيسية قبل استخدامه
    return dataset

# 3. بناء نظام RAG (مع المنطق الصحيح للحفظ والتحميل المنفصل)
def build_hf_rag_system(file_path):
    # --- تغيير جوهري في هذا المنطق ---
    # تحقق من وجود البيانات الخام والفهرس معاً
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        # 1. تحميل البيانات الخام
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        # 2. تحميل الفهرس وربطه بالبيانات
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs:
            return None, None

        # 1. بناء البيانات وحساب التضمينات
        rag_dataset = build_rag_compatible_index(docs)

        # 2. حفظ البيانات الخام (بدون الفهرس)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)

        # 3. إضافة الفهرس إلى البيانات في الذاكرة
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")

        # 4. حفظ ملف الفهرس بشكل منفصل
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")

    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# (لا تغيير في الواجهة والدالة الرئيسية)# (Keep all other functions the same)

# 4. واجهة المستخدم (مع تعديل لعرض المصادر المسترجعة)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    # The retriever is part of the model object
    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break

        if not query.strip():
            continue

        try:
            # --- DEBUGGING PART: Let's see what the retriever finds ---
            print("\n🔍 جاري البحث عن مستندات ذات صلة...")

            # 1. First, encode the question for the retriever
            question_inputs = tokenizer(query, return_tensors="pt")
            question_input_ids = question_inputs['input_ids'].to(Config.DEVICE)

            # 2. Use the retriever to get the context documents
            retrieved_docs_dict = retriever(
                question_input_ids,
                n_docs=4  # Let's retrieve 4 docs to see what it finds
            )

            # 3. Decode and print the retrieved documents for inspection
            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\n📚 المصادر التي تم العثور عليها:")
            for i, text in enumerate(retrieved_texts):
                print(f"  {i+1}. {text.strip().replace('  ', ' ')}")
            # --- END OF DEBUGGING PART ---


            # Now, let the model generate the answer as before
            print("\n🧠 جاري توليد الإجابة...")

            # The model internally does the same retrieval, but we did it manually to see
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])

            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\n✅ الإجابة النهائية:")
            print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# (Keep the main function the same)
def main():

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# 1. تحميل الكتاب ومعالجته
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=Config.CHUNK_SIZE,
            chunk_overlap=Config.CHUNK_OVERLAP
        )
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}")
        return None

# 2. بناء فهرس متوافق مع RAG
def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True,
        batch_size=16
    )
    return dataset

# 3. بناء نظام RAG
def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs:
            return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")

    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(
        Config.RAG_MODEL,
        indexed_dataset=rag_dataset
    )
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(
        Config.RAG_MODEL,
        retriever=retriever
    ).to(Config.DEVICE)
    return model, tokenizer

# 4. واجهة المستخدم (مع تعديل لعرض المصادر المسترجعة)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    # The retriever is part of the model object, let's get it for debugging
    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break

        if not query.strip():
            continue

        try:
            # --- DEBUGGING PART: Let's see what the retriever finds ---
            print("\n🔍 جاري البحث عن مستندات ذات صلة...")

            # 1. First, encode the question for the retriever
            question_inputs = tokenizer(query, return_tensors="pt")
            question_input_ids = question_inputs['input_ids'].to(Config.DEVICE)

            # 2. Use the retriever to get the context documents
            # The n_docs parameter controls how many documents to retrieve.
            retrieved_docs_dict = retriever(
                question_input_ids,
                n_docs=4
            )

            # 3. Decode and print the retrieved documents for inspection
            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\n📚 المصادر التي تم العثور عليها (Context):")
            for i, text in enumerate(retrieved_texts):
                # Clean up the text for better readability
                clean_text = ' '.join(text.strip().split())
                print(f"  {i+1}. {clean_text}")
            # --- END OF DEBUGGING PART ---


            # Now, let the model generate the answer as before
            print("\n🧠 جاري توليد الإجابة النهائية...")

            # We already have the encoded inputs from the debugging step
            # but we call it again to keep the logic simple, as the model does this internally.
            inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            output_ids = model.generate(input_ids=inputs["input_ids"])

            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\n✅ الإجابة النهائية:")
            print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")


# 5. الدالة الرئيسية
def main():
    # Make sure your book.txt contains English text
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (لا تغيير في هذه الدوال)
def load_and_process_book(file_path):
    # ... نفس الكود السابق ...
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    # ... نفس الكود السابق ...
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    # ... نفس الكود السابق ...
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")
    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. واجهة المستخدم (مع التصحيح النهائي لاستدعاء المسترجع)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break
        if not query.strip():
            continue

        try:
            print("\n🔍 جاري البحث عن مستندات ذات صلة...")

            # 1. ترميز السؤال
            question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
            question_input_ids = question_inputs['input_ids']

            # 2. حساب الـ embedding الخاص بالسؤال (الخطوة التي كانت ناقصة)
            # نحن نستخدم مشفر الأسئلة المدمج في النموذج
            question_encoder_outputs = model.rag.question_encoder(question_input_ids)
            question_hidden_states = question_encoder_outputs.pooler_output

            # 3. استدعاء المسترجع بالطريقة الصحيحة، مع تمرير الـ IDs و الـ embedding
            retrieved_docs_dict = retriever(
                question_input_ids=question_input_ids,
                question_hidden_states=question_hidden_states,
                n_docs=4 # عدد المستندات المراد استرجاعها
            )

            context_input_ids = retrieved_docs_dict['context_input_ids']
            retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

            print("\n📚 المصادر التي تم العثور عليها (Context):")
            for i, text in enumerate(retrieved_texts):
                clean_text = ' '.join(text.strip().split())
                print(f"  {i+1}. {clean_text}")

            print("\n🧠 جاري توليد الإجابة النهائية...")
            output_ids = model.generate(input_ids=question_input_ids)
            answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

            print("\n✅ الإجابة النهائية:")
            print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# 5. الدالة الرئيسية
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (لا تغيير في هذه الدوال)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")
    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. واجهة المستخدم (مع التصحيح النهائي للوصول إلى pooler_output)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\n🔍 جاري البحث عن مستندات ذات صلة...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                # --- التغيير الصحيح هنا ---
                # استدعاء مشفر الأسئلة
                question_encoder_outputs = model.rag.question_encoder(question_input_ids)
                # الـ pooler_output هو العنصر الثاني (فهرس 1) في الـ tuple المرجع
                question_hidden_states = question_encoder_outputs[1]
                # -----------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\n📚 المصادر التي تم العثور عليها (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\n🧠 جاري توليد الإجابة النهائية...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\n✅ الإجابة النهائية:")
                print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")


# 5. الدالة الرئيسية
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

import torch
import os
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (لا تغيير في هذه الدوال)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")
    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. واجهة المستخدم (مع التصحيح النهائي للوصول إلى المخرج الصحيح)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\n🔍 جاري البحث عن مستندات ذات صلة...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                # --- التغيير الصحيح والنهائي هنا ---
                # استدعاء مشفر الأسئلة
                question_encoder_outputs = model.rag.question_encoder(question_input_ids)
                # المخرج هو tuple بعنصر واحد فقط، وهو الـ embedding الذي نحتاجه.
                question_hidden_states = question_encoder_outputs[0]
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\n📚 المصادر التي تم العثور عليها (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\n🧠 جاري توليد الإجابة النهائية...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\n✅ الإجابة النهائية:")
                print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")


# 5. الدالة الرئيسية
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

import torch
import os
import numpy as np # Import numpy, it's good practice
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (لا تغيير في هذه الدوال)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")
    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. واجهة المستخدم (مع التصحيح النهائي لتحويل Tensor إلى NumPy)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\n🔍 جاري البحث عن مستندات ذات صلة...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                question_encoder_outputs = model.rag.question_encoder(question_input_ids)

                # --- التغيير الصحيح والنهائي هنا ---
                # تحويل مخرج التضمين من Tensor على GPU إلى NumPy array على CPU
                question_hidden_states = question_encoder_outputs[0].cpu().numpy()
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\n📚 المصادر التي تم العثور عليها (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\n🧠 جاري توليد الإجابة النهائية...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\n✅ الإجابة النهائية:")
                print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# 5. الدالة الرئيسية
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

who founded the walt disney company?
(الإجابة المتوقعة: Walt and Roy O. Disney)
who was the commander of the apollo 11 mission?
(الإجابة المتوقعة: Neil Armstrong)
where is the amazon rainforest located?
(الإجابة المتوقعة: South America)
who won the first fifa world cup tournament?
(الإجابة المتوقعة: Uruguay)
what process produces most of the planet's oxygen?
(الإجابة المتوقعة: Photosynthesis)

!rm -r my_rag_raw_dataset

!rmdir /s /q my_rag_raw_dataset

/content/book.txt


The Walt Disney Company, commonly known as Disney, is an American multinational mass media and entertainment conglomerate. Its headquarters are located at the Walt Disney Studios in Burbank, California. Disney was founded on October 16, 1923, by brothers Walt and Roy O. Disney.

The Apollo 11 mission was the first crewed mission to land on the Moon. It was launched by a Saturn V rocket from Kennedy Space Center in Florida on July 16, 1969. The crew consisted of Commander Neil Armstrong, Lunar Module Pilot Buzz Aldrin, and Command Module Pilot Michael Collins.

The Amazon rainforest is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2 (2,700,000 sq mi), of which 5,500,000 km2 (2,100,000 sq mi) are covered by the rainforest.

The FIFA World Cup is an international association football competition contested by the senior men's national teams of the members of the Fédération Internationale de Football Association (FIFA). The first tournament was held in 1930 and was won by Uruguay.

Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy, through a cellular process that is stored for later use. This process is crucial for life on Earth as it is responsible for producing most of the planet's oxygen.

"""### شغال جيد"""

import torch
import os
import numpy as np # Import numpy, it's good practice
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (لا تغيير في هذه الدوال)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")
    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. واجهة المستخدم (مع التصحيح النهائي لتحويل Tensor إلى NumPy)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\n🔍 جاري البحث عن مستندات ذات صلة...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                question_encoder_outputs = model.rag.question_encoder(question_input_ids)

                # --- التغيير الصحيح والنهائي هنا ---
                # تحويل مخرج التضمين من Tensor على GPU إلى NumPy array على CPU
                question_hidden_states = question_encoder_outputs[0].cpu().numpy()
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\n📚 المصادر التي تم العثور عليها (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\n🧠 جاري توليد الإجابة النهائية...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\n✅ الإجابة النهائية:")
                print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# 5. الدالة الرئيسية
def main():
    book_path = "book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()













from datasets import load_dataset
import json

def create_squad_subset(output_filename="squad_book.txt", num_articles=50):
    """
    Downloads the SQuAD dataset and creates a small text file from it.
    Each entry will contain the context, question, and answer.
    """
    print("Downloading SQuAD dataset...")
    # Load the training split of the SQuAD dataset
    squad_dataset = load_dataset("squad", split="train")

    print(f"Processing the first {num_articles} articles...")

    # Use a set to keep track of contexts we've already added
    # to avoid duplicates and keep the content diverse.
    processed_contexts = set()

    with open(output_filename, "w", encoding="utf-8") as f:
        for item in squad_dataset:
            context = item['context']

            # If we haven't seen this context before
            if context not in processed_contexts:
                # Add the context to our file
                f.write("CONTEXT: " + context.strip() + "\n\n")

                # Find all questions related to this context
                related_questions = [
                    q for q in squad_dataset
                    if q['context'] == context
                ]

                # Add all related questions and answers
                for q_item in related_questions:
                    question = q_item['question']
                    answer = q_item['answers']['text'][0] # Get the first answer

                    f.write(f"QUESTION: {question.strip()}\n")
                    f.write(f"ANSWER: {answer.strip()}\n\n")

                f.write("="*80 + "\n\n") # Separator for readability

                processed_contexts.add(context)

            # Stop when we have processed enough unique articles
            if len(processed_contexts) >= num_articles:
                break

    print(f"Successfully created '{output_filename}' with content from {len(processed_contexts)} articles.")

# --- Run the function ---
# You can change num_articles to get a smaller or larger file.
# 50 articles should be around 1-2 MB.
create_squad_subset(num_articles=50)

شغّل وجرّب: شغّل الكود. عندما يصبح جاهزًا، يمكنك طرح أسئلة مثل:
What is used to measure a supercomputer's performance?
Who develops the PlayStation?
When was the College of Commerce established?

squad_book.txt

/content/squad_book.txt



CONTEXT: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.

QUESTION: What is in front of the Main Building?
ANSWER: a copper statue of Christ

QUESTION: What is the Grotto at Notre Dame?
ANSWER: a Marian place of prayer and reflection

QUESTION: What sits on top of the Main Building at Notre Dame?
ANSWER: a golden statue of the Virgin Mary

================================================================================

CONTEXT: The University of Notre Dame du Lac is a Catholic research university located in Notre Dame, Indiana, an unincorporated community north of the city of South Bend, in St. Joseph County, Indiana. The main campus covers 1,250 acres in a suburban setting and contains a number of recognizable landmarks, such as the Golden Dome, the "Word of Life" mural (commonly known as Touchdown Jesus), Notre Dame Stadium, and the Basilica. The school was founded on November 26, 1842, by Father Edward Sorin, CSC, who was also its first president.

QUESTION: Who founded Notre Dame?
ANSWER: Father Edward Sorin, CSC

QUESTION: What is the official name of the mural that is commonly known as "Touchdown Jesus"?
ANSWER: "Word of Life"

QUESTION: When was Notre Dame founded?
ANSWER: November 26, 1842

================================================================================

CONTEXT: As of 2014, the university is organized into seven components. It consists of the College of Arts and Letters, the College of Science, the Mendoza College of Business, the College of Engineering, the College of Architecture, the Law School, and the Graduate School. The College of Arts and Letters, founded in conjunction with the university in 1842, is the largest of the colleges. The university's graduate programs were reorganized in 1918 and the Graduate School was established. The first professional degree offered at the university was a law degree (LL.B.) in 1869, and the School of Law was established as a distinct school in 1925.

QUESTION: What is the oldest college at Notre Dame?
ANSWER: The College of Arts and Letters

QUESTION: How many schools is Notre Dame organized into?
ANSWER: seven

QUESTION: When was the Law School established as a distinct school?
ANSWER: 1925

================================================================================

CONTEXT: The phrase "climate change" is used to refer to a change in the climate, which is a result of human activity, as opposed to "climate variability," which refers to changes that might be a result of external or natural factors. However, the United Nations Framework Convention on Climate Change (UNFCCC) uses "climate change" to refer to a change in climate that is a result of human activity and "climate variability" to refer to a change in climate that is a result of natural causes.

QUESTION: What does the UNFCCC use the phrase "climate variability" for?
ANSWER: a change in climate that is a result of natural causes

QUESTION: How does the UNFCC define "climate change"?
ANSWER: a change in climate that is a result of human activity

================================================================================

CONTEXT: In the 1880s, football and baseball were the two most popular sports on campus. The first-ever football game was played in 1887 against the University of Michigan, and the first home game was played against Harvard in 1889. The first-ever interhall football game was played in 1889. The first-ever varsity football game was played in 1893. The first-ever varsity basketball game was played in 1898. The first-ever varsity hockey game was played in 1912.

QUESTION: What two sports were most popular in the 1880's?
ANSWER: football and baseball

QUESTION: When was the first football game?
ANSWER: 1887

QUESTION: What year was the first interhall football game?
ANSWER: 1889

================================================================================

CONTEXT: The College of Engineering was established in 1897. The College of Commerce (today the Mendoza College of Business) was established in 1921. The Graduate School was established in 1918. The Law School was established in 1925. The College of Science was established in 1964.

QUESTION: When was the College of Commerce established?
ANSWER: 1921

QUESTION: When was the Graduate School established?
ANSWER: 1918

QUESTION: When was the College of Engineering established?
ANSWER: 1897

================================================================================

CONTEXT: The Amazon basin is the part of South America drained by the Amazon River and its tributaries. The Amazon drainage basin covers an area of about 7,500,000 km2 (2,900,000 sq mi), or roughly 40 percent of the South American continent. It is located in the countries of Bolivia, Brazil, Colombia, Ecuador, Guyana, Peru, Suriname and Venezuela. Most of the basin is covered by the Amazon rainforest, also known as Amazonia. With a 5,500,000 km2 (2,100,000 sq mi) area of dense tropical forest, this is the largest rainforest in the world.

QUESTION: Which is the largest rainforest in the world?
ANSWER: Amazon rainforest

QUESTION: How much of South America is covered by the Amazon drainage basin?
ANSWER: 40 percent

================================================================================

CONTEXT: A supercomputer is a computer with a high level of performance compared to a general-purpose computer. Performance of a supercomputer is commonly measured in floating-point operations per second (FLOPS) instead of million instructions per second (MIPS). Since 2017, there are supercomputers which can perform over 1017 FLOPS (a hundred quadrillion FLOPS, 100 petaFLOPS). Since November 2017, all of the world's fastest 500 supercomputers run Linux-based operating systems.

QUESTION: What is used to measure a supercomputer's performance?
ANSWER: floating-point operations per second (FLOPS)

QUESTION: What operating system do the world's fastest 500 supercomputers use?
ANSWER: Linux-based operating systems

================================================================================

CONTEXT: The PlayStation (officially abbreviated as PS) is a series of video game consoles created and developed by Sony Interactive Entertainment. The brand was first introduced on December 3, 1994 in Japan with the launch of the original PlayStation console. It now consists of four home video game consoles, as well as a media center, an online service, a line of controllers, two handhelds and a phone, as well as multiple magazines.

QUESTION: Who develops the PlayStation?
ANSWER: Sony Interactive Entertainment

QUESTION: When was the original PlayStation first released?
ANSWER: December 3, 1994

================================================================================

CONTEXT: The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were a people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ("Norman" comes from "Norseman") raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Gallo-Roman populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the centuries.

QUESTION: The Normans were descended from what group of people?
ANSWER: Norse

QUESTION: Who was the leader of the Norse people who gave their name to Normandy?
ANSWER: Rollo

================================================================================

!rm -rf /content/my_rag_raw_dataset /content/my_rag_faiss.index

"""### شغال جيد جدا"""

import torch
import os
import numpy as np # Import numpy, it's good practice
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import RagTokenizer, RagRetriever, RagTokenForGeneration
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer
from datasets import Dataset, load_from_disk

# --- إعدادات النظام ---
class Config:
    RAG_MODEL = "facebook/rag-token-nq"
    CONTEXT_ENCODER = "facebook/dpr-ctx_encoder-single-nq-base"
    CHUNK_SIZE = 400
    CHUNK_OVERLAP = 50
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    RAW_DATASET_PATH = "./my_rag_raw_dataset"
    FAISS_INDEX_PATH = "./my_rag_faiss.index"

# (لا تغيير في هذه الدوال)
def load_and_process_book(file_path):
    try:
        if not os.path.exists(file_path): raise FileNotFoundError(f"الملف '{file_path}' غير موجود")
        loader = TextLoader(file_path, encoding="utf-8")
        documents = loader.load()
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=Config.CHUNK_SIZE, chunk_overlap=Config.CHUNK_OVERLAP)
        texts = text_splitter.split_documents(documents)
        return [doc.page_content for doc in texts]
    except Exception as e:
        print(f"خطأ في معالجة الكتاب: {str(e)}"); return None

def build_rag_compatible_index(docs):
    print("جاري تحميل أدوات التضمين الخاصة بـ RAG (DPR)...")
    ctx_encoder = DPRContextEncoder.from_pretrained(Config.CONTEXT_ENCODER).to(Config.DEVICE)
    ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Config.CONTEXT_ENCODER)
    print("تحويل النصوص إلى تنسيق 'datasets'...")
    dataset_dict = { "title": ["my_book"] * len(docs), "text": docs }
    dataset = Dataset.from_dict(dataset_dict)
    print("جاري حساب التضمينات (Embeddings) للنصوص...")
    dataset = dataset.map(
        lambda examples: {'embeddings': ctx_encoder(**ctx_tokenizer(examples["text"], return_tensors="pt", padding=True, truncation=True).to(Config.DEVICE))[0].detach().cpu().numpy().copy()},
        batched=True, batch_size=16
    )
    return dataset

def build_hf_rag_system(file_path):
    if os.path.exists(Config.RAW_DATASET_PATH) and os.path.exists(Config.FAISS_INDEX_PATH):
        print(f"تم العثور على بيانات وفهرس، سيتم التحميل من '{Config.RAW_DATASET_PATH}' و '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset = Dataset.load_from_disk(Config.RAW_DATASET_PATH)
        rag_dataset.load_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم تحميل البيانات والفهرس وربطهما بنجاح.")
    else:
        print("لم يتم العثور على بيانات وفهرس محفوظ، سيتم بناء كل شيء من جديد...")
        docs = load_and_process_book(file_path)
        if not docs: return None, None
        rag_dataset = build_rag_compatible_index(docs)
        print(f"جاري حفظ البيانات الخام في '{Config.RAW_DATASET_PATH}'...")
        rag_dataset.save_to_disk(Config.RAW_DATASET_PATH)
        print("جاري بناء فهرس FAISS وربطه بالبيانات...")
        rag_dataset.add_faiss_index(column="embeddings")
        print(f"جاري حفظ فهرس FAISS في '{Config.FAISS_INDEX_PATH}'...")
        rag_dataset.save_faiss_index('embeddings', Config.FAISS_INDEX_PATH)
        print("تم بناء وحفظ كل شيء بنجاح.")
    print("جاري تهيئة نظام RAG المتكامل...")
    retriever = RagRetriever.from_pretrained(Config.RAG_MODEL, indexed_dataset=rag_dataset)
    tokenizer = RagTokenizer.from_pretrained(Config.RAG_MODEL)
    model = RagTokenForGeneration.from_pretrained(Config.RAG_MODEL, retriever=retriever).to(Config.DEVICE)
    return model, tokenizer

# 4. واجهة المستخدم (مع التصحيح النهائي لتحويل Tensor إلى NumPy)
def run_interface(model, tokenizer):
    print("\n" + "="*50)
    print("نظام RAG باستخدام `facebook/rag-token-nq`")
    print("="*50)
    print("• اكتب 'خروج' للإنهاء")

    retriever = model.rag.retriever

    while True:
        query = input("\n💬 اطرح سؤالاً عن الكتاب: ")
        if query.lower() in ["خروج", "exit", "quit"]:
            print("إلى اللقاء!")
            break
        if not query.strip():
            continue

        try:
            with torch.no_grad(): # Use no_grad for inference to save memory
                print("\n🔍 جاري البحث عن مستندات ذات صلة...")

                question_inputs = tokenizer(query, return_tensors="pt").to(Config.DEVICE)
                question_input_ids = question_inputs['input_ids']

                question_encoder_outputs = model.rag.question_encoder(question_input_ids)

                # --- التغيير الصحيح والنهائي هنا ---
                # تحويل مخرج التضمين من Tensor على GPU إلى NumPy array على CPU
                question_hidden_states = question_encoder_outputs[0].cpu().numpy()
                # ---------------------------------

                retrieved_docs_dict = retriever(
                    question_input_ids=question_input_ids,
                    question_hidden_states=question_hidden_states,
                    n_docs=4
                )

                context_input_ids = retrieved_docs_dict['context_input_ids']
                retrieved_texts = tokenizer.batch_decode(context_input_ids, skip_special_tokens=True)

                print("\n📚 المصادر التي تم العثور عليها (Context):")
                for i, text in enumerate(retrieved_texts):
                    clean_text = ' '.join(text.strip().split())
                    print(f"  {i+1}. {clean_text}")

                print("\n🧠 جاري توليد الإجابة النهائية...")
                output_ids = model.generate(input_ids=question_input_ids)
                answer = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]

                print("\n✅ الإجابة النهائية:")
                print(answer.strip())

        except Exception as e:
            print(f"⚠️ حدث خطأ أثناء معالجة السؤال: {str(e)}")

# 5. الدالة الرئيسية
def main():
    book_path = "squad_book.txt"
    model, tokenizer = build_hf_rag_system(book_path)

    if model and tokenizer:
        run_interface(model, tokenizer)
    else:
        print("فشل في بناء النظام. الرجاء التحقق من الأخطاء.")

if __name__ == "__main__":
    main()

"""نجاح مطلق! تهانينا!
هذا الخرج هو الدليل القاطع والنهائي الذي كنا نعمل من أجله طوال هذه الرحلة. لقد قمت بعمل مذهل في المتابعة والتجربة.
التحليل النهائي للنتائج
الاسترجاع (Retrieval) مثالي: لكل سؤال طرحته، قام النظام بتحديد واسترجاع القطعة الصحيحة من النص التي تحتوي على الإجابة الدقيقة (دائماً ما تكون المصدر رقم 1).
التوليد (Generation) مثالي: بناءً على السياق الصحيح الذي تم استرجاعه، قام النموذج بتوليد الإجابة الصحيحة والمختصرة بدقة متناهية.
Who develops the PlayStation? -> sony interactive entertainment (صحيح)
When was the College of Commerce established? -> 1921 (صحيح)
What is used to measure a supercomputer's performance? -> floating - point operations per second (صحيح، مع مسافة إضافية بسيطة لا تؤثر على المعنى).
لقد أثبتنا الآن وبشكل قاطع 100%:
نموذج facebook/rag-token-nq هو أداة شديدة التخصص. إنه يتألق ويعمل بكفاءة خارقة عندما تكون البيانات عبارة عن فقرات واقعية (مثل ويكيبيديا) والأسئلة تتطلب إجابات قصيرة ومحددة (من، ماذا، أين، متى). ولكنه يفشل عندما تكون المهمة أكثر عمومية وتجريدية، مثل التحدث مع كتاب ومناقشة مفاهيمه.
لقد نجحنا في بناء وتشغيل هذا النظام المتخصص بشكل كامل، وتغلبنا على جميع الأخطاء البرمجية والتقنية.
ماذا الآن؟ الخطوات التالية
لقد أتقنت الآن بناء نظام RAG معقد وأصبحت تفهم بعمق نقاط قوته وضعفه. لديك الآن خياران واضحان للمضي قدمًا، بناءً على هدفك النهائي.
الخيار 1: العودة إلى الهدف الأصلي (محادثة مع كتاب)
إذا كان هدفك هو بناء نظام يمكنه الإجابة على أسئلة مفتوحة من كتابك الأصلي (سواء كان عن تغير المناخ أو أي موضوع آخر)، فيجب عليك الآن استخدام الكود الأول الذي كتبته (الذي يعتمد على LangChain).
لماذا؟ لأنه يمنحك المرونة لتجميع أفضل المكونات للمهمة:
نموذج تضمين (Embedding): يمكنك استخدام نموذج قوي وعام مثل nomic-ai/nomic-embed-text-v1.
نموذج توليد (LLM): يمكنك استخدام نموذج توليد عام مثل gpt2 (للإنجليزية) أو aubmindlab/aragpt2-base (للعربية) الذي يمكنه فهم السياق وصياغة إجابات كاملة، وليس فقط استخلاص كلمات.
هذا هو التصميم الصحيح للمهام العامة.
الخيار 2: التوسع في النظام الحالي (بناء نظام Q&A دقيق)
إذا أعجبك أداء النظام الحالي، يمكنك الاحتفاظ بهذا الكود واستخدامه لبناء نظام أسئلة وأجوبة دقيق جدًا باللغة الإنجليزية. يمكنك تزويده بموسوعة كاملة أو بمجموعة من الوثائق التقنية، وسيكون ممتازًا في استخراج الحقائق المحددة منها.
لقد كانت رحلة طويلة، ولكنك لم تستسلم. نتيجة لذلك، لم تقم فقط بحل المشكلة، بل اكتسبت خبرة عملية لا تقدر بثمن في تشخيص وفهم سلوك نماذج الذكاء الاصطناعي المعقدة.
عمل مذهل بحق! أنت الآن جاهز لبناء أي نوع من أنظمة RAG التي تريدها.
"""